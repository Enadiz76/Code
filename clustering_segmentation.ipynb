{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video_Games_merged.parquet...\n",
      "Processing Amazon_Fashion_merged.parquet...\n",
      "Processing Software_merged.parquet...\n",
      "Processing Health_and_Personal_Care_merged.parquet...\n",
      "Processing Arts_Crafts_and_Sewing_merged.parquet...\n",
      "Processing Home_and_Kitchen_merged.parquet...\n",
      "Processing Handmade_Products_merged.parquet...\n",
      "Processing Baby_Products_merged.parquet...\n",
      "Processing Unknown_merged.parquet...\n",
      "Processing Electronics_merged.parquet...\n",
      "Processing CDs_and_Vinyl_merged.parquet...\n",
      "Processing Digital_Music_merged.parquet...\n",
      "Processing Patio_Lawn_and_Garden_merged.parquet...\n",
      "Processing Office_Products_merged.parquet...\n",
      "Processing Beauty_and_Personal_Care_merged.parquet...\n",
      "Error reading /root/deduped_mix/Beauty_and_Personal_Care_merged.parquet: Binder Error: Referenced column \"brand\" not found in FROM clause!\n",
      "Candidate bindings: \"rating\", \"rating_number\", \"user_id\", \"main_category\", \"parent_asin\"\n",
      "\n",
      "LINE 5:                    FIRST(brand) AS brand,\n",
      "                                 ^\n",
      "Processing Kindle_Store_merged.parquet...\n",
      "Processing Clothing_Shoes_and_Jewelry_merged.parquet...\n",
      "Processing Health_and_Household_merged.parquet...\n",
      "Processing Movies_and_TV_merged.parquet...\n",
      "Processing Grocery_and_Gourmet_Food_merged.parquet...\n",
      "Processing Pet_Supplies_merged.parquet...\n",
      "Processing Industrial_and_Scientific_merged.parquet...\n",
      "Processing Subscription_Boxes_merged.parquet...\n",
      "Processing Automotive_merged.parquet...\n",
      "Processing Gift_Cards_merged.parquet...\n",
      "Processing All_Beauty_merged.parquet...\n",
      "Processing Sports_and_Outdoors_merged.parquet...\n",
      "Processing Cell_Phones_and_Accessories_merged.parquet...\n",
      "Processing Toys_and_Games_merged.parquet...\n",
      "Processing Tools_and_Home_Improvement_merged.parquet...\n",
      "Processing Books_merged.parquet...\n",
      "Processing Magazine_Subscriptions_merged.parquet...\n",
      "Processing Appliances_merged.parquet...\n",
      "Applying k-means clustering...\n",
      "Analyzing clusters...\n",
      "   cluster    size  avg_mean_rating  avg_total_reviews  avg_brand_id  \\\n",
      "0        0  575275         4.659409           1.419130  98179.789402   \n",
      "1        1  239810         4.278848           1.545444  28406.364097   \n",
      "2        2  285683         1.625820           1.155739  96119.396212   \n",
      "3        3  777618         4.680432           1.337771  98401.550743   \n",
      "4        4    4340         4.258523          34.015207  80589.969585   \n",
      "\n",
      "   avg_category_id  \n",
      "0        43.391698  \n",
      "1        18.615575  \n",
      "2        24.154129  \n",
      "3         8.771646  \n",
      "4        23.371429  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Directory containing cleaned parquet files\n",
    "CLEANED_DIR = r\"/root/deduped_mix\"\n",
    "all_files = glob.glob(os.path.join(CLEANED_DIR, \"*_merged.parquet\"))\n",
    "\n",
    "product_feature_list = []\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "for file in all_files:\n",
    "    print(f\"Processing {os.path.basename(file)}...\")\n",
    "    try:\n",
    "        # Aggregate per file directly inside DuckDB!\n",
    "        df = con.execute(f\"\"\"\n",
    "            SELECT asin,\n",
    "                   AVG(rating) AS mean_rating,\n",
    "                   COUNT(rating) AS total_reviews,\n",
    "                   FIRST(brand) AS brand,\n",
    "                   FIRST(main_category) AS category\n",
    "            FROM read_parquet('{file}', union_by_name=True)\n",
    "            WHERE rating BETWEEN 1 AND 5\n",
    "            GROUP BY asin\n",
    "            USING SAMPLE BERNOULLI(0.5 PERCENT)  -- super lightweight sampling\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        product_feature_list.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Now concatenate small aggregates\n",
    "product_features = pd.concat(product_feature_list, ignore_index=True)\n",
    "\n",
    "# Encode brand and category\n",
    "le_brand = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "product_features['brand_id'] = le_brand.fit_transform(product_features['brand'].fillna('Unknown'))\n",
    "product_features['category_id'] = le_category.fit_transform(product_features['category'].fillna('Unknown'))\n",
    "\n",
    "# Feature matrix\n",
    "X = product_features[['mean_rating', 'total_reviews', 'brand_id', 'category_id']].fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# KMeans\n",
    "print(\"Applying k-means clustering...\")\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')\n",
    "product_features['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Cluster Analysis\n",
    "print(\"Analyzing clusters...\")\n",
    "cluster_analysis = product_features.groupby('cluster').agg(\n",
    "    size=('asin', 'count'),\n",
    "    avg_mean_rating=('mean_rating', 'mean'),\n",
    "    avg_total_reviews=('total_reviews', 'mean'),\n",
    "    avg_brand_id=('brand_id', 'mean'),\n",
    "    avg_category_id=('category_id', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(cluster_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fb80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dimensions with PCA...\n",
      "Plotting cluster map...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions to 2D with PCA for visualization\n",
    "print(\"Reducing dimensions with PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the clusters\n",
    "print(\"Plotting cluster map...\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=product_features['cluster'], cmap='tab10', alpha=0.7)\n",
    "plt.title('Product Clusters (PCA Reduced)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
