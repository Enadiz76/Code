{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d9d035",
   "metadata": {},
   "source": [
    "# COMP 3610 – A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d06bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark\n",
    "# %pip install findspark\n",
    "# %pip install -q gdown\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install pyarrow\n",
    "# %pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66dda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "# import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# import `StandardScaler`\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# sudo apt install python3-distutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "645f1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "332558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "# import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# import `StandardScaler`\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# sudo apt install python3-distutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd04f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    ".appName(\"Amazon_Reviews\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c33973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"parent_asin\", FloatType(), True),\n",
    "    StructField(\"user_id\", ArrayType(StringType()), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True),\n",
    "    StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    StructField(\"helpful_vote\", StringType(), True),\n",
    "])\n",
    "\n",
    "# String types in arrays may need to be sequence but couldn't find the actual sequence dytpe syntax\n",
    "meta_schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", FloatType(), True),\n",
    "    StructField(\"rating_number\", IntegerType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType()), True),\n",
    "    StructField(\"description\", ArrayType(StringType()), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True),\n",
    "    StructField(\"videos\", ArrayType(StringType()), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"categories\", ArrayType(StringType()), True),\n",
    "    StructField(\"details\", MapType(StringType(), IntegerType()), True),\n",
    "    StructField(\"parent_asin\", FloatType(), True),\n",
    "    StructField(\"user_id\", ArrayType(StringType()), True),\n",
    "    StructField(\"bought_together\", ArrayType(StringType()), True),\n",
    "    # StructField(\"timestamp\", IntegerType(), True),\n",
    "    # StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    # StructField(\"helpful_vote\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a217ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_folder = 'root/Data'\n",
    "output_folder = 'root/output_folder'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9aeafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed93461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, batch_size=1000):\n",
    "    temp_path = \"root/Data/temp_extract\"\n",
    "    if os.path.exists(temp_path):\n",
    "        shutil.rmtree(temp_path)\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"Extracting tar files...\")\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            output_path = os.path.join(output_folder, \"meta.parquet\" if is_meta else \"reviews.parquet\")\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            print(f\"Streaming {arrow_file.name} → {output_path}\")\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            for i, row in enumerate(dataset):\n",
    "                batch.append(row)\n",
    "                if len(batch) >= batch_size:\n",
    "                    table = pa.Table.from_pylist(batch)\n",
    "                    pq.write_to_dataset(table, root_path=output_path)\n",
    "                    print(f\"Wrote batch of {len(batch)} rows to {output_path}\")\n",
    "                    batch = []\n",
    "\n",
    "            if batch:\n",
    "                table = pa.Table.from_pylist(batch)\n",
    "                pq.write_to_dataset(table, root_path=output_path)  # ✅ removed append=True\n",
    "                print(f\"Wrote final batch of {len(batch)} rows to {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40019db",
   "metadata": {},
   "source": [
    "Calling fn to preprocess for a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d2bfa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar files...\n",
      "Error: File Data/raw_meta_Amazon_Fashion.tar.bz2 does not exist.\n",
      "Error: File Data/raw_review_Amazon_Fashion.tar.bz2 does not exist.\n",
      "Found 0 Arrow files\n"
     ]
    }
   ],
   "source": [
    "preprocess_category(\"Data/raw_meta_Amazon_Fashion.tar.bz2\",\n",
    "                    \"Data/raw_review_Amazon_Fashion.tar.bz2\",\n",
    "                    output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9220f981",
   "metadata": {},
   "source": [
    "Load the parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eefb961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"root/output_folder/reviews.parquet\")\n",
    "meta = pd.read_parquet(\"root/output_folder/meta.parquet\")\n",
    "# table = pq.read_table(\"root/output_folder/meta.parquet\")\n",
    "# meta = table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa5c94",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af27297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading Parquet files...\n",
      " Reviews shape: (0, 0)\n",
      " Metadata shape: (0, 0)\n",
      " Merging on parent_asin...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'parent_asin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_31010/3595311778.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m print(f\" Metadata shape: {meta.shape}\")\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#  2. Merge on 'parent_asin'\u001b[39;00m\n\u001b[32m      9\u001b[39m print(\u001b[33m\" Merging on parent_asin...\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m merged = pd.merge(reviews, meta, on=\u001b[33m\"parent_asin\"\u001b[39m, how=\u001b[33m\"inner\"\u001b[39m, suffixes=(\u001b[33m\"_review\"\u001b[39m, \u001b[33m\"_meta\"\u001b[39m))\n\u001b[32m     11\u001b[39m print(f\" Merged shape: {merged.shape}\")\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#  3a. Drop rows with invalid or missing ratings\u001b[39;00m\n",
      "\u001b[32m~/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32m~/venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'parent_asin'"
     ]
    }
   ],
   "source": [
    "#  1. Load the reviews and metadata\n",
    "print(\" Loading Parquet files...\")\n",
    "reviews = pd.read_parquet(\"root/output_folder/reviews.parquet\")\n",
    "meta = pd.read_parquet(\"root/output_folder/meta.parquet\")\n",
    "print(f\" Reviews shape: {reviews.shape}\")\n",
    "print(f\" Metadata shape: {meta.shape}\")\n",
    "\n",
    "#  2. Merge on 'parent_asin'\n",
    "print(\" Merging on parent_asin...\")\n",
    "merged = pd.merge(reviews, meta, on=\"parent_asin\", how=\"inner\", suffixes=(\"_review\", \"_meta\"))\n",
    "print(f\" Merged shape: {merged.shape}\")\n",
    "\n",
    "#  3a. Drop rows with invalid or missing ratings\n",
    "print(\" Filtering invalid ratings...\")\n",
    "merged = merged[merged[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "#  3b. Drop rows with empty or missing review text\n",
    "print(\" Dropping empty review text...\")\n",
    "merged = merged[merged[\"text\"].notna() & (merged[\"text\"].str.strip() != \"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63611d",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with brand  data\n",
    "# 3c. Extract brand (from details or store) or set to \"Unknown\"\n",
    "print(\" Extracting brand from metadata...\")\n",
    "merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "#  4. Remove duplicates: (user id, asin, text)\n",
    "print(\" Removing duplicate reviews...\")\n",
    "merged.drop_duplicates(subset=[\"user id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "#  5a. Derived column: review_length (token count)\n",
    "print(\" Computing review length...\")\n",
    "merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "#  5b. Derived column: year (from timestamp)\n",
    "print(\" Extracting year from timestamp...\")\n",
    "merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"s\", errors=\"coerce\").dt.year\n",
    "\n",
    "#  6. Save cleaned data\n",
    "output_path = \"root/output_folder/cleaned_merged.parquet\"\n",
    "print(f\" Saving cleaned dataset to: {output_path}\")\n",
    "merged.to_parquet(output_path, index=False)\n",
    "\n",
    "print(\" All cleaning steps completed.\")\n",
    "print(f\" Final dataset shape: {merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c30a3",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74773e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_meta = Dataset.from_file(\"/root/Code/root/Data/temp_extract/raw_meta_Gift_Cards/full/data-00000-of-00001.arrow\")\n",
    "# ds_review = Dataset.from_file(\"/root/Code/root/Data/temp_extract/raw_review_Gift_Cards/full/data-00000-of-00001.arrow\")\n",
    "# ds_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc4c2a",
   "metadata": {},
   "source": [
    "Dead Code (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c95544",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Combine review files\n",
    "    # review_files = [f for f in arrow_files if \"meta\" not in str(f)]\n",
    "    # combined_review_file = f\"{temp_path}/combined_reviews.arrow\"\n",
    "    # print(combined_review_file)\n",
    "    # combine_arrow_files(review_files, combined_review_file)\n",
    "\n",
    "    # # Combine meta files\n",
    "    # meta_files = [f for f in arrow_files if \"meta\" in str(f)]\n",
    "    # combined_meta_file = f\"{temp_path}/combined_meta.arrow\"\n",
    "    # print(combined_meta_file)\n",
    "    # combine_arrow_files(meta_files, combined_meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, size, split, year, from_unixtime, when, lit\n",
    "# from datasets import Dataset\n",
    "# import pyarrow.json as pajson\n",
    "# import pyarrow.dataset as ds\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import json\n",
    "\n",
    "\n",
    "# def preprocess_category(review_tar_path, meta_tar_path, output_folder):\n",
    "#     temp_path = \"root/Data/temp_extract\"\n",
    "#     if os.path.exists(temp_path):\n",
    "#         shutil.rmtree(temp_path)\n",
    "#     os.makedirs(temp_path, exist_ok=True)\n",
    "\n",
    "#     print(\"attempting to call extract function...\")\n",
    "#     extract_tar_bz2(review_tar_path, temp_path)\n",
    "#     extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "#     # finding the json files and reading\n",
    "#     print(\"Finding Arrow files...\")\n",
    "#     arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "#     print(f\"Found Arrow files: {arrow_files}\")\n",
    "\n",
    "#     # print the length of the arrow files\n",
    "#     print(len(arrow_files))\n",
    "#     review_file = []\n",
    "#     meta_file = []\n",
    "\n",
    "#     # review_file.append([f for f in arrow_files if \"meta\" not in str(f)][0])\n",
    "#     # meta_file = [f for f in arrow_files if \"meta\" in str(f)][0]\n",
    "\n",
    "#     # print(review_file)\n",
    "#     test_df = pd.DataFrame()\n",
    "\n",
    "#     for arrow_file in arrow_files:\n",
    "#         try:\n",
    "#             table = Dataset.from_file(str(arrow_file))\n",
    "#             test_df = table.to_pandas()\n",
    "#             print(\"Successful!\")\n",
    "#     #         test_df = spark.createDataFrame(table.to_pandas())\n",
    "#     #         if \"meta\" in str(arrow_file):\n",
    "#     #             metadata_frames.append(df)\n",
    "#     #         else:\n",
    "#     #             review_frames.append(df)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file {arrow_file}: {e}\")\n",
    "\n",
    "#     print(test_df)\n",
    "    \n",
    "\n",
    "#     # Combine all metadata and review dataframes\n",
    "#     # metadata_df = metadata_frames[0]\n",
    "#     # for frame in metadata_frames[1:]:\n",
    "#     #     metadata_df = metadata_df.union(frame)\n",
    "\n",
    "#     # review_df = review_frames[0]\n",
    "#     # for frame in review_frames[1:]:\n",
    "#     #     review_df = review_df.union(frame)\n",
    "\n",
    "\n",
    "\n",
    "#     # # Load the combined JSON files into Spark DataFrames\n",
    "#     # reviews_df = spark.read.json(combined_review_file)\n",
    "#     # meta_df = spark.read.json(combined_meta_file)\n",
    "\n",
    "#     # reviews_df.show()\n",
    "#     # meta_df.show()\n",
    "#     # Load with pyarrow and convert to Spark DataFrame\n",
    "#     # reviews_df = spark.createDataFrame(pajson.read_json(str(review_file)).to_pandas())  # Use pyarrow to read JSON\n",
    "#     # meta_df = spark.createDataFrame(pajson.read_json(str(meta_file)).to_pandas())  # Use pyarrow to read JSON\n",
    "#     # reviews_df = spark.read.schema(review_schema).json(str(review_file))  # Use pyarrow to read JSON\n",
    "#     # meta_df = spark.read.schema(meta_schema).json(str(meta_file))  # Use pyarrow to read JSON\n",
    "\n",
    "#     # Assuming 'asin' is present in both DataFrames, we join on it\n",
    "#     # df = reviews_df.join(meta_df, on='parent_asin', how='inner') \n",
    "\n",
    "#     # # # load with Spark\n",
    "#     # reviews_df = spark.read.json(str(review_file))\n",
    "#     # meta_df = spark.read.json(str(meta_file))\n",
    "\n",
    "#     # # print(reviews_df)\n",
    "#     # reviews_df.show()\n",
    "\n",
    "#     # # merge on 'parent_asin'\n",
    "#     # df = reviews_df.join(meta_df, on='parent_asin', how='inner')\n",
    "\n",
    "#     # # drop rows with invalid ratings or empty text\n",
    "#     # df = df.filter((col(\"rating\").between(1, 5)) &\n",
    "#     #                (col(\"text\").isNotNull()) &\n",
    "#     #                (col(\"text\") != \"\"))\n",
    "\n",
    "#     # # brand logic\n",
    "#     # df = df.withColumn(\n",
    "#     #     \"brand\",\n",
    "#     #     when(col(\"details.brand\").isNotNull(), col(\"details.brand\"))\n",
    "#     #     .when(col(\"store\").isNotNull(), col(\"store\"))\n",
    "#     #     .otherwise(lit(\"Unknown\"))\n",
    "#     # )\n",
    "\n",
    "#     # # derived columns\n",
    "#     # df = df.withColumn(\"review_length\", size(split(col(\"text\"), \" \")))\n",
    "#     # df = df.withColumn(\"year\", year(from_unixtime(col(\"timestamp\"))))\n",
    "\n",
    "#     # # drop duplicates (based on user_id, asin, text)\n",
    "#     # df = df.dropDuplicates([\"user_id\", \"asin\", \"text\"])\n",
    "\n",
    "#     # # save as Parquet\n",
    "#     # category = Path(review_tar_path).stem.replace(\"raw_review_\", \"\")\n",
    "#     # output_file = os.path.join(output_folder, f\"cleaned_{category}.parquet\")\n",
    "#     # df.write.mode(\"overwrite\").parquet(output_file)\n",
    "\n",
    "#     # shutil.rmtree(temp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c681d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# def combine_json_files(json_files, output_file):\n",
    "#     combined_data = []\n",
    "#     for file in json_files:\n",
    "#         with open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "#             try:\n",
    "#                 data = json.load(f)\n",
    "#                 if isinstance(data, list):  # If the JSON is an array\n",
    "#                     combined_data.extend(data)\n",
    "#                 else:  # If the JSON is an object\n",
    "#                     combined_data.append(data)\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"Error decoding JSON from file {file}: {e}\")\n",
    "    \n",
    "#     # Save the combined data to a new JSON file\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(combined_data, f, indent=4)\n",
    "#     print(f\"Combined JSON saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
