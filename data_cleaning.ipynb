{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "base_directory = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\"\n",
    "raw_files_dir = os.path.join(base_directory, \"raw_files\")\n",
    "cleaned_pars = os.path.join(base_directory, \"raw_files\",\"cleaned_parquet\")\n",
    "os.makedirs(raw_files_dir, exist_ok=True)\n",
    "os.makedirs(cleaned_pars, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def extract_tar_bz2_files(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar.bz2\"):\n",
    "            filepath = os.path.join(directory, file)\n",
    "            extract_dir = os.path.join(directory, file.replace(\".tar.bz2\", \"\"))\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Extracting: {file}\")\n",
    "            with tarfile.open(filepath, \"r:bz2\") as tar:\n",
    "                tar.extractall(path=extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4458979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: raw_meta_Gift_Cards.tar.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maian\\AppData\\Local\\Temp\\ipykernel_39824\\154044346.py:13: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: raw_review_Gift_Cards.tar.bz2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extract_tar_bz2_files(raw_files_dir)\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mextract_tar_bz2_files\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr:bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[1;32m---> 13\u001b[0m     tar\u001b[38;5;241m.\u001b[39mextractall(path\u001b[38;5;241m=\u001b[39mextract_dir)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\tarfile.py:2302\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[1;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[0;32m   2297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[0;32m   2298\u001b[0m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[0;32m   2299\u001b[0m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[0;32m   2300\u001b[0m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[0;32m   2301\u001b[0m         directories\u001b[38;5;241m.\u001b[39mappend(tarinfo)\n\u001b[1;32m-> 2302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_one(tarinfo, path, set_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir(),\n\u001b[0;32m   2303\u001b[0m                       numeric_owner\u001b[38;5;241m=\u001b[39mnumeric_owner)\n\u001b[0;32m   2305\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[0;32m   2306\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\tarfile.py:2365\u001b[0m, in \u001b[0;36mTarFile._extract_one\u001b[1;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_member(tarinfo, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, tarinfo\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2366\u001b[0m                          set_attrs\u001b[38;5;241m=\u001b[39mset_attrs,\n\u001b[0;32m   2367\u001b[0m                          numeric_owner\u001b[38;5;241m=\u001b[39mnumeric_owner)\n\u001b[0;32m   2368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_fatal_error(e)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\tarfile.py:2448\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[1;32m-> 2448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakefile(tarinfo, targetpath)\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[0;32m   2450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\tarfile.py:2502\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[1;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[0;32m   2500\u001b[0m     target\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[0;32m   2501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2502\u001b[0m     copyfileobj(source, target, tarinfo\u001b[38;5;241m.\u001b[39msize, ReadError, bufsize)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\tarfile.py:251\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[0;32m    249\u001b[0m blocks, remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(length, bufsize)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(blocks):\n\u001b[1;32m--> 251\u001b[0m     buf \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(bufsize)\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m bufsize:\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\bz2.py:164\u001b[0m, in \u001b[0;36mBZ2File.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read up to size uncompressed bytes from the file.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mIf size is negative or omitted, read until EOF is reached.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03mReturns b'' if the file is already at EOF.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_can_read()\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\maian\\anaconda3\\Lib\\_compression.py:97\u001b[0m, in \u001b[0;36mDecompressReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mneeds_input:\n\u001b[1;32m---> 97\u001b[0m         rawblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(BUFFER_SIZE)\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rawblock:\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompressed file ended before the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend-of-stream marker was reached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extract_tar_bz2_files(raw_files_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ba484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Gift_Cards...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df712c2be81b4c83af90ecc255ad7cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/153 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a4266ecb094ef699dbc70ca87fca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "def arrow_to_parquet(category):\n",
    "    try:\n",
    "        review_dict = load_from_disk(os.path.join(raw_files_dir, f\"raw_review_{category}\"))\n",
    "        meta_dict = load_from_disk(os.path.join(raw_files_dir, f\"raw_meta_{category}\"))\n",
    "\n",
    "        # Extract the 'full' split\n",
    "        review_ds = review_dict[\"full\"]\n",
    "        meta_ds = meta_dict[\"full\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {category} — failed to load arrow or 'full' split: {e}\")\n",
    "        return\n",
    "\n",
    "    # Save to Parquet\n",
    "    r_folder = os.path.join(cleaned_pars, f\"{category}_review.parquet\")\n",
    "    m_folder = os.path.join(cleaned_pars, f\"{category}_meta.parquet\")\n",
    "    print(f\"Exporting {category}...\")\n",
    "\n",
    "    review_ds.to_parquet(r_folder)\n",
    "    meta_ds.to_parquet(m_folder)\n",
    "\n",
    "#Detect categories with both review and meta folders\n",
    "categories = [\n",
    "    name.replace(\"raw_review_\", \"\")\n",
    "    for name in os.listdir(raw_files_dir)\n",
    "    if name.startswith(\"raw_review_\")\n",
    "    and os.path.isdir(os.path.join(raw_files_dir, name))\n",
    "    and os.path.isdir(os.path.join(raw_files_dir, f\"raw_meta_{name.replace('raw_review_', '')}\"))\n",
    "]\n",
    "\n",
    "categories = ['Gift_Cards']\n",
    "\n",
    "for cat in categories:\n",
    "    arrow_to_parquet(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39254134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "    review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "        print(f\"Skipping {category} — one of the parquet files is missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Merging: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE meta AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "                THEN json_extract(details, '$.brand')::STRING\n",
    "                ELSE 'Unknown'\n",
    "            END AS brand\n",
    "        FROM '{meta_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    # No deduplication\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE cleaned AS\n",
    "        SELECT\n",
    "            r.user_id,\n",
    "            r.asin,\n",
    "            r.parent_asin,\n",
    "            r.rating,\n",
    "            r.text,\n",
    "            r.verified_purchase,\n",
    "            r.helpful_vote,\n",
    "            array_length(string_split(r.text, ' ')) AS review_length,\n",
    "             strftime(\n",
    "                    CASE \n",
    "                        WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "                        ELSE to_timestamp(CAST(r.timestamp AS DOUBLE) / 1000.0)\n",
    "                    END,\n",
    "                    '%Y'\n",
    "                )::INTEGER AS year,\n",
    "            m.brand,\n",
    "            m.main_category,\n",
    "            m.title,\n",
    "            m.average_rating,\n",
    "            m.rating_number,\n",
    "            m.price,\n",
    "            '{category}' AS category\n",
    "        FROM review r\n",
    "        LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "        WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"COPY cleaned TO '{output_path}' (FORMAT PARQUET);\")\n",
    "    con.close()\n",
    "    print(f\"Saved merged and cleaned data → {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f653c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['Automotive', 'Grocery_and_Gourmet_Food', 'Handmade_Products', 'Health_and_Household', 'Health_and_Personal_Care', 'Home_and_Kitchen', 'Industrial_and_Scientific', 'Kindle_Store', 'Magazine_Subscriptions', 'Movies_and_TV', 'Musical_Instruments']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Only look for categories with review + meta parquet files\n",
    "def get_parquet_categories(cleaned_pars):\n",
    "    review_files = set(\n",
    "        f.replace(\"_review.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_pars) if f.endswith(\"_review.parquet\")\n",
    "    )\n",
    "    meta_files = set(\n",
    "        f.replace(\"_meta.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_pars) if f.endswith(\"_meta.parquet\")\n",
    "    )\n",
    "    return sorted(review_files & meta_files)\n",
    "\n",
    "categories = get_parquet_categories(cleaned_pars)\n",
    "print(\"Found categories:\", categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting/Resuming merge for: Grocery_and_Gourmet_Food\n",
      "📦 Writing chunk 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a2aef86be64140935817ec42a69b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories =['Home_and_Kitchen']\n",
    "for cat in categories:\n",
    "    merge_parquet_to_duckdb(cat, cleaned_pars)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec29e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_parquet(input_path, output_path):\n",
    "    import duckdb\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    print(f\"Deduplicating: {input_path}\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE deduplicated AS\n",
    "        SELECT * EXCLUDE(row_num)\n",
    "        FROM (\n",
    "            SELECT *,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY user_id, asin, text\n",
    "                    ORDER BY year\n",
    "                ) AS row_num\n",
    "            FROM read_parquet('{input_path}')\n",
    "        )\n",
    "        WHERE row_num = 1;\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"COPY deduplicated TO '{output_path}' (FORMAT PARQUET);\")\n",
    "    con.close()\n",
    "    print(f\"Saved deduplicated data → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d066f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧼 Deduplicating: C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\\raw_files\\cleaned_parquet\\temp_duckdb.db.tmp\\Automotive_merged.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2882ab5ac81447bb98f4b0d33589b156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46a4c5cd1fe4a0fa79a63c022dc22b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved deduplicated data → C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\\raw_files\\cleaned_parquet\\Automotive_dedup.parquet\n"
     ]
    }
   ],
   "source": [
    "input = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\\raw_files\\cleaned_parquet\\temp_duckdb.db.tmp\\Automotive_merged.parquet\"\n",
    "output = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\\raw_files\\cleaned_parquet\\Automotive_dedup.parquet\"\n",
    "\n",
    "\n",
    "deduplicate_parquet(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de517bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "def count_parquet_rows(parquet_path):\n",
    "    con = duckdb.connect()\n",
    "    result = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{parquet_path}');\").fetchone()[0]\n",
    "    con.close()\n",
    "    print(f\"🧮 Total rows: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7cdaaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Total rows: 19955450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19955450"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\\raw_files\\cleaned_parquet\\temp_duckdb.db.tmp\\Automotive_merged.parquet\"\n",
    "\n",
    "count_parquet_rows(path1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
