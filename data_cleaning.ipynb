{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import glob\n",
    "import tarfile\n",
    "import re\n",
    "import duckdb\n",
    "\n",
    "BASE_DIR = r\"D:\\AS3\\Last\"\n",
    "RAW_DIR = BASE_DIR\n",
    "CLEANED_DIR = r\"D:\\AS3\\Last\\Last paqs\"\n",
    "MERGED_DIR = r\"D:\\AS3\\Cleaned\\merged\"\n",
    "\n",
    "# os.makedirs(RAW_DIR, exist_ok=True)\n",
    "# os.makedirs(CLEANED_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def extract_tar_bz2_files(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar.bz2\"):\n",
    "            filepath = os.path.join(directory, file)\n",
    "            extract_dir = os.path.join(directory, file.replace(\".tar.bz2\", \"\"))\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Extracting: {file}\")\n",
    "            with tarfile.open(filepath, \"r:bz2\") as tar:\n",
    "                tar.extractall(path=extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4458979",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tar_bz2_files(RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_and_merge_streaming(category):\n",
    "    review_path = os.path.join(RAW_DIR, f\"raw_review_{category}\", \"data.jsonl\")\n",
    "    meta_path = os.path.join(RAW_DIR, f\"raw_meta_{category}\", \"data.jsonl\")\n",
    "\n",
    "    try:\n",
    "        meta_df = pd.read_json(meta_path, lines=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Meta load failed for {category}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Preprocess metadata\n",
    "    meta_df[\"brand\"] = meta_df[\"details\"].apply(lambda d: d.get(\"brand\") if isinstance(d, dict) else \"Unknown\")\n",
    "    meta_df[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    save_path = os.path.join(CLEANED_DIR, f\"{category}.parquet\")\n",
    "    chunk_iter = pd.read_json(review_path, lines=True, chunksize=100_000)\n",
    "    \n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"[{category}] Processing chunk {i+1}\")\n",
    "\n",
    "        # Filter and clean\n",
    "        chunk = chunk.dropna(subset=[\"rating\", \"text\"])\n",
    "        chunk = chunk[chunk[\"rating\"].between(1, 5)]\n",
    "        chunk = chunk.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "        chunk[\"review_length\"] = chunk[\"text\"].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "        chunk[\"year\"] = pd.to_datetime(chunk[\"timestamp\"], unit='ms', errors='coerce').dt.year\n",
    "\n",
    "        # Merge\n",
    "        merged = pd.merge(chunk, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "        # Append to parquet\n",
    "        if not os.path.exists(save_path):\n",
    "            merged.to_parquet(save_path, index=False)\n",
    "        else:\n",
    "            merged.to_parquet(save_path, index=False, append=True)\n",
    "\n",
    "    print(f\"Saved {category} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_merge_streaming(\"Office_Products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ba484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# import os\n",
    "\n",
    "\n",
    "def arrow_to_parquet(category):\n",
    "    try:\n",
    "        review_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_review_{category}\"))\n",
    "        meta_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_meta_{category}\"))\n",
    "\n",
    "        review_ds = review_dict[\"full\"]\n",
    "        meta_ds = meta_dict[\"full\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {category} â€” failed to load arrow or 'full' split: {e}\")\n",
    "        return\n",
    "\n",
    "    review_path = os.path.join(CLEANED_DIR, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(CLEANED_DIR, f\"{category}_meta.parquet\")\n",
    "    print(f\"Exporting {category}...\")\n",
    "\n",
    "    review_ds.to_parquet(review_path)\n",
    "    meta_ds.to_parquet(meta_path)\n",
    "\n",
    "categories = [\n",
    "    name.replace(\"raw_review_\", \"\")\n",
    "    for name in os.listdir(RAW_DIR)\n",
    "    if name.startswith(\"raw_review_\")\n",
    "    and os.path.isdir(os.path.join(RAW_DIR, name))\n",
    "    and os.path.isdir(os.path.join(RAW_DIR, f\"raw_meta_{name.replace('raw_review_', '')}\"))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655393b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "    arrow_to_parquet(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39254134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "#     review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "#     meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "#     output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "#     if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "#         print(f\"âš ï¸ Skipping {category} â€” one of the parquet files is missing.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Merging: {category}\")\n",
    "\n",
    "#     con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "\n",
    "#     con.execute(f\"CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\")\n",
    "\n",
    "#     con.execute(f\"\"\"\n",
    "#         CREATE OR REPLACE TABLE meta AS\n",
    "#         SELECT *,\n",
    "#             CASE\n",
    "#                 WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "#                 THEN json_extract(details, '$.brand')::STRING\n",
    "#                 ELSE 'Unknown'\n",
    "#             END AS brand\n",
    "#         FROM '{meta_path}';\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Deduplication via ROW_NUMBER\n",
    "#     con.execute(f\"\"\"\n",
    "#         CREATE OR REPLACE TABLE cleaned AS\n",
    "#         SELECT * EXCLUDE(row_num)\n",
    "#         FROM (\n",
    "#             SELECT\n",
    "#                 r.user_id,\n",
    "#                 r.asin,\n",
    "#                 r.parent_asin,\n",
    "#                 r.rating,\n",
    "#                 r.text,\n",
    "#                 r.verified_purchase,\n",
    "#                 r.helpful_vote,\n",
    "#                 array_length(string_split(r.text, ' ')) AS review_length,\n",
    "#                 strftime(\n",
    "#                     CASE \n",
    "#                         WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "#                         ELSE to_timestamp(CAST(r.timestamp AS DOUBLE) / 1000.0)\n",
    "#                     END,\n",
    "#                     '%Y'\n",
    "#                 )::INTEGER AS year,\n",
    "#                 m.brand,\n",
    "#                 m.main_category,\n",
    "#                 m.title,\n",
    "#                 m.average_rating,\n",
    "#                 m.rating_number,\n",
    "#                 m.price,\n",
    "#                 '{category}' AS category,\n",
    "#                 ROW_NUMBER() OVER (\n",
    "#                     PARTITION BY r.user_id, r.asin, r.text\n",
    "#                     ORDER BY r.timestamp\n",
    "#                 ) AS row_num\n",
    "#             FROM review r\n",
    "#             LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "#             WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL\n",
    "#         )\n",
    "#         WHERE row_num = 1;\n",
    "#     \"\"\")\n",
    "\n",
    "#     con.execute(f\"COPY cleaned TO '{output_path}' (FORMAT PARQUET);\")\n",
    "#     con.close()\n",
    "#     print(f\"Saved merged and cleaned data â†’ {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0f75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "    review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "        print(f\"âš  Skipping {category} â€” one of the parquet files is missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Merging: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "\n",
    "    con.execute(\"PRAGMA max_temp_directory_size='1TiB';\")\n",
    "\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE meta AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "                THEN json_extract(details, '$.brand')::STRING\n",
    "                ELSE 'Unknown'\n",
    "            END AS brand\n",
    "        FROM '{meta_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    # No deduplication\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE cleaned AS\n",
    "        SELECT\n",
    "            r.user_id,\n",
    "            r.asin,\n",
    "            r.parent_asin,\n",
    "            r.rating,\n",
    "            r.text,\n",
    "            r.verified_purchase,\n",
    "            r.helpful_vote,\n",
    "            array_length(string_split(r.text, ' ')) AS review_length,\n",
    "             strftime(\n",
    "                    CASE \n",
    "                        WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "                        ELSE to_timestamp(CAST(r.timestamp AS DOUBLE) / 1000.0)\n",
    "                    END,\n",
    "                    '%Y'\n",
    "                )::INTEGER AS year,\n",
    "            m.brand,\n",
    "            m.main_category,\n",
    "            m.title,\n",
    "            m.average_rating,\n",
    "            m.rating_number,\n",
    "            m.price,\n",
    "            '{category}' AS category\n",
    "        FROM review r\n",
    "        LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "        WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"COPY cleaned TO '{output_path}' (FORMAT PARQUET);\")\n",
    "    con.close()\n",
    "    print(f\"Saved merged and cleaned data â†’ {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f653c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['Kindle_Store']\n"
     ]
    }
   ],
   "source": [
    "def get_parquet_categories(cleaned_dir):\n",
    "    review_files = set(\n",
    "        f.replace(\"_review.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_review.parquet\")\n",
    "    )\n",
    "    meta_files = set(\n",
    "        f.replace(\"_meta.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_meta.parquet\")\n",
    "    )\n",
    "    return sorted(review_files & meta_files)\n",
    "\n",
    "categories = get_parquet_categories(CLEANED_DIR)\n",
    "print(\"Found categories:\", categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f04b60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: Kindle_Store\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e1b4ef034a443e99394899a0977597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9132d46034504b7a9571e843a13768cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1870466fa78f4186a77862b8c1c43c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fbb580ae4b4fb6b011e01ca35dafbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged and cleaned data â†’ D:\\AS3\\Last\\Last paqs\\Kindle_Store_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    merge_parquet_to_duckdb(cat, CLEANED_DIR)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deduplicate_parquet(input_path, output_path):\n",
    "#     import duckdb\n",
    "\n",
    "#     con = duckdb.connect()\n",
    "#     print(f\"Deduplicating: {input_path}\")\n",
    "\n",
    "#     con.execute(f\"\"\"\n",
    "#         CREATE TABLE deduplicated AS\n",
    "#         SELECT * EXCLUDE(row_num)\n",
    "#         FROM (\n",
    "#             SELECT *,\n",
    "#                 ROW_NUMBER() OVER (\n",
    "#                     PARTITION BY user_id, asin, text\n",
    "#                     ORDER BY year\n",
    "#                 ) AS row_num\n",
    "#             FROM read_parquet('{input_path}')\n",
    "#         )\n",
    "#         WHERE row_num = 1;\n",
    "#     \"\"\")\n",
    "\n",
    "#     con.execute(f\"COPY deduplicated TO '{output_path}' (FORMAT PARQUET);\")\n",
    "#     con.close()\n",
    "#     print(f\"Saved deduplicated data â†’ {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9423013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL_DIR = os.path.join(MERGED_DIR, \"Pet_Supplies_merged.parquet\" )\n",
    "# DEDUP_DIR = r\"D:\\AS3\\Final\\Pet_Supplies_merged.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276eae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate_parquet(FINAL_DIR,DEDUP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572d138",
   "metadata": {},
   "source": [
    "Worked for small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75843ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deduplicate_folder_parquets(input_dir, output_dir, dedup_columns, order_column='year'):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     all_parquets = glob.glob(os.path.join(input_dir, \"*.parquet\"))\n",
    "\n",
    "#     print(f\"Found {len(all_parquets)} files to deduplicate...\\n\")\n",
    "\n",
    "#     for input_path in all_parquets:\n",
    "#         filename = os.path.basename(input_path)\n",
    "#         name_wo_ext = filename.replace(\".parquet\", \"\")\n",
    "#         output_path = os.path.join(output_dir, f\"{name_wo_ext}_deduped.parquet\")\n",
    "\n",
    "#         if os.path.exists(output_path):\n",
    "#             print(f\"Skipping (already done): {filename}\")\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Deduplicating: {filename} ...\")\n",
    "\n",
    "#         try:\n",
    "#             con = duckdb.connect()\n",
    "#             dedup_key = ', '.join(dedup_columns)\n",
    "\n",
    "#             con.execute(f\"\"\"\n",
    "#                 CREATE TABLE deduplicated AS\n",
    "#                 SELECT * EXCLUDE(row_num)\n",
    "#                 FROM (\n",
    "#                     SELECT *,\n",
    "#                         ROW_NUMBER() OVER (\n",
    "#                             PARTITION BY {dedup_key}\n",
    "#                             ORDER BY {order_column}\n",
    "#                         ) AS row_num\n",
    "#                     FROM read_parquet('{input_path}')\n",
    "#                 )\n",
    "#                 WHERE row_num = 1;\n",
    "#             \"\"\")\n",
    "\n",
    "#             con.execute(f\"\"\"\n",
    "#                 COPY deduplicated TO '{output_path}' (FORMAT PARQUET);\n",
    "#             \"\"\")\n",
    "\n",
    "#             print(f\"Saved to {output_path}\\n\")\n",
    "#             con.close()\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with {filename}: {e}\\n\")\n",
    "\n",
    "#     print(\"\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08aa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate_folder_parquets(\n",
    "#     input_dir=r\"D:\\AS3\\Cleaned\\merged\",\n",
    "#     output_dir=r\"D:\\AS3\\Final\",\n",
    "#     dedup_columns=[\"user_id\", \"asin\", \"text\"],\n",
    "#     order_column=\"year\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deduplicate_large_parquet_chunked(con, input_path, dedup_columns, order_column='year', chunk_size=250_000):\n",
    "#     print(f\"ðŸ§  Using chunked deduplication for large file: {os.path.basename(input_path)}\")\n",
    "\n",
    "#     con.execute(\"DROP TABLE IF EXISTS temp_all;\")\n",
    "#     con.execute(f\"\"\"\n",
    "#         CREATE TABLE temp_all AS\n",
    "#         SELECT * FROM read_parquet('{input_path}', union_by_name=true)\n",
    "#         LIMIT 0;\n",
    "#     \"\"\")\n",
    "\n",
    "#     offset = 0\n",
    "#     while True:\n",
    "#         chunk_df = con.execute(f\"\"\"\n",
    "#             SELECT * FROM read_parquet('{input_path}', union_by_name=true)\n",
    "#             LIMIT {chunk_size} OFFSET {offset}\n",
    "#         \"\"\").fetchdf()\n",
    "\n",
    "#         if chunk_df.empty:\n",
    "#             break\n",
    "\n",
    "#         con.register(\"chunk\", chunk_df)\n",
    "#         con.execute(\"INSERT INTO temp_all SELECT * FROM chunk\")\n",
    "#         print(f\"âœ… Chunk {offset // chunk_size + 1} inserted ({len(chunk_df)} rows)\")\n",
    "#         offset += chunk_size\n",
    "\n",
    "#     dedup_key = ', '.join(dedup_columns)\n",
    "#     con.execute(\"DROP TABLE IF EXISTS deduplicated;\")\n",
    "#     con.execute(f\"\"\"\n",
    "#         CREATE TABLE deduplicated AS\n",
    "#         SELECT * EXCLUDE(row_num)\n",
    "#         FROM (\n",
    "#             SELECT *,\n",
    "#                 ROW_NUMBER() OVER (\n",
    "#                     PARTITION BY {dedup_key}\n",
    "#                     ORDER BY {order_column}\n",
    "#                 ) AS row_num\n",
    "#             FROM temp_all\n",
    "#         )\n",
    "#         WHERE row_num = 1;\n",
    "#     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def deduplicate_folder_merged_files(input_dir, output_dir, dedup_columns, order_column='year', chunk_threshold_gb=2.0):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     all_parquets = glob.glob(os.path.join(input_dir, \"*_merged.parquet\"))\n",
    "\n",
    "#     print(f\"Found {len(all_parquets)} '_merged' files to deduplicate...\\n\")\n",
    "\n",
    "#     for input_path in all_parquets:\n",
    "#         filename = os.path.basename(input_path)\n",
    "#         name_wo_ext = filename.replace(\"_merged.parquet\", \"\")\n",
    "#         output_path = os.path.join(output_dir, f\"{name_wo_ext}_deduped.parquet\")\n",
    "\n",
    "#         if os.path.exists(output_path):\n",
    "#             print(f\"Skipping (already deduplicated): {filename}\")\n",
    "#             continue\n",
    "\n",
    "#         file_size_gb = os.path.getsize(input_path) / (1024 ** 3)\n",
    "#         con = duckdb.connect()\n",
    "\n",
    "#         try:\n",
    "#             if file_size_gb >= chunk_threshold_gb:\n",
    "#                 deduplicate_large_parquet_chunked(\n",
    "#                     con,\n",
    "#                     input_path,\n",
    "#                     dedup_columns=dedup_columns,\n",
    "#                     order_column=order_column,\n",
    "#                     chunk_size=250_000\n",
    "#                 )\n",
    "#             else:\n",
    "#                 print(f\"Deduplicating: {filename} (size: {file_size_gb:.2f} GB)\")\n",
    "#                 dedup_key = ', '.join(dedup_columns)\n",
    "\n",
    "#                 con.execute(\"DROP TABLE IF EXISTS deduplicated;\")\n",
    "#                 con.execute(f\"\"\"\n",
    "#                     CREATE TABLE deduplicated AS\n",
    "#                     SELECT * EXCLUDE(row_num)\n",
    "#                     FROM (\n",
    "#                         SELECT *,\n",
    "#                             ROW_NUMBER() OVER (\n",
    "#                                 PARTITION BY {dedup_key}\n",
    "#                                 ORDER BY {order_column}\n",
    "#                             ) AS row_num\n",
    "#                         FROM read_parquet('{input_path}', union_by_name=true)\n",
    "#                     )\n",
    "#                     WHERE row_num = 1;\n",
    "#                 \"\"\")\n",
    "\n",
    "#             con.execute(f\"COPY deduplicated TO '{output_path}' (FORMAT PARQUET);\")\n",
    "#             print(f\"Deduplicated and saved to: {output_path}\\n\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with {filename}: {e}\\n\")\n",
    "#         finally:\n",
    "#             con.close()\n",
    "\n",
    "#     print(\"All deduplication complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  deduplicate_folder_merged_files(\n",
    "#     input_dir=r\"D:\\AS3\\Cleaned\\merged\",\n",
    "#     output_dir=r\"D:\\AS3\\Final\",\n",
    "#     dedup_columns=[\"user_id\", \"asin\", \"text\"],\n",
    "#     order_column=\"year\",\n",
    "#     chunk_threshold_gb=2.0  # Files over 2 GB will trigger chunking\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cat in categories:\n",
    "#     if cat == \"Tools_and_Home_Improvement\":\n",
    "#         deduplicate_folder_parquets(cat, MERGED_DIR, overwrite=True)\n",
    "#         gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
