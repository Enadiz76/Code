{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\New folder\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw_files\")\n",
    "CLEANED_DIR = os.path.join(BASE_DIR, \"cleaned_parquet\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def extract_tar_bz2_files(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar.bz2\"):\n",
    "            filepath = os.path.join(directory, file)\n",
    "            extract_dir = os.path.join(directory, file.replace(\".tar.bz2\", \"\"))\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Extracting: {file}\")\n",
    "            with tarfile.open(filepath, \"r:bz2\") as tar:\n",
    "                tar.extractall(path=extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4458979",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\New folder\\raw_files\"\n",
    "extract_tar_bz2_files(RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_and_merge_streaming(category):\n",
    "    review_path = os.path.join(RAW_DIR, f\"raw_review_{category}\", \"data.jsonl\")\n",
    "    meta_path = os.path.join(RAW_DIR, f\"raw_meta_{category}\", \"data.jsonl\")\n",
    "\n",
    "    try:\n",
    "        meta_df = pd.read_json(meta_path, lines=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Meta load failed for {category}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Preprocess metadata\n",
    "    meta_df[\"brand\"] = meta_df[\"details\"].apply(lambda d: d.get(\"brand\") if isinstance(d, dict) else \"Unknown\")\n",
    "    meta_df[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    save_path = os.path.join(CLEANED_DIR, f\"{category}.parquet\")\n",
    "    chunk_iter = pd.read_json(review_path, lines=True, chunksize=100_000)\n",
    "    \n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"[{category}] Processing chunk {i+1}\")\n",
    "\n",
    "        # Filter and clean\n",
    "        chunk = chunk.dropna(subset=[\"rating\", \"text\"])\n",
    "        chunk = chunk[chunk[\"rating\"].between(1, 5)]\n",
    "        chunk = chunk.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "        chunk[\"review_length\"] = chunk[\"text\"].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "        chunk[\"year\"] = pd.to_datetime(chunk[\"timestamp\"], unit='s', errors='coerce').dt.year\n",
    "\n",
    "        # Merge\n",
    "        merged = pd.merge(chunk, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "        # Append to parquet\n",
    "        if not os.path.exists(save_path):\n",
    "            merged.to_parquet(save_path, index=False)\n",
    "        else:\n",
    "            merged.to_parquet(save_path, index=False, append=True)\n",
    "\n",
    "    print(f\"Saved {category} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ba484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "def arrow_to_parquet(category):\n",
    "    try:\n",
    "        review_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_review_{category}\"))\n",
    "        meta_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_meta_{category}\"))\n",
    "\n",
    "        # Extract the 'full' split\n",
    "        review_ds = review_dict[\"full\"]\n",
    "        meta_ds = meta_dict[\"full\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {category} ‚Äî failed to load arrow or 'full' split: {e}\")\n",
    "        return\n",
    "\n",
    "    # Save to Parquet\n",
    "    review_path = os.path.join(CLEANED_DIR, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(CLEANED_DIR, f\"{category}_meta.parquet\")\n",
    "    print(f\"Exporting {category}...\")\n",
    "\n",
    "    review_ds.to_parquet(review_path)\n",
    "    meta_ds.to_parquet(meta_path)\n",
    "\n",
    "# Detect categories with both review and meta folders\n",
    "# categories = [\n",
    "#     name.replace(\"raw_review_\", \"\")\n",
    "#     for name in os.listdir(RAW_DIR)\n",
    "#     if name.startswith(\"raw_review_\")\n",
    "#     and os.path.isdir(os.path.join(RAW_DIR, name))\n",
    "#     and os.path.isdir(os.path.join(RAW_DIR, f\"raw_meta_{name.replace('raw_review_', '')}\"))\n",
    "# ]\n",
    "\n",
    "categories = ['Home_and_Kitchen','Industrial_and_Scientific']\n",
    "\n",
    "for cat in categories:\n",
    "    arrow_to_parquet(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39254134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "    review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping {category} ‚Äî one of the parquet files is missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîÑ Merging: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    #load metadata parquet with brand extraction\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE meta AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "                THEN json_extract(details, '$.brand')::STRING\n",
    "                ELSE 'Unknown'\n",
    "            END AS brand\n",
    "        FROM '{meta_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    #clean and transform, without deduplication\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE cleaned AS\n",
    "        SELECT\n",
    "            r.user_id,\n",
    "            r.asin,\n",
    "            r.parent_asin,\n",
    "            r.rating,\n",
    "            r.text,\n",
    "            r.verified_purchase,\n",
    "            r.helpful_vote,\n",
    "            array_length(string_split(r.text, ' ')) AS review_length,\n",
    "            strftime('%Y',\n",
    "                CASE \n",
    "                    WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "                    ELSE CAST(to_timestamp(CAST(r.timestamp AS DOUBLE)) AS TIMESTAMP)\n",
    "                END\n",
    "            )::INTEGER AS year,\n",
    "            m.brand,\n",
    "            m.main_category,\n",
    "            m.title,\n",
    "            m.average_rating,\n",
    "            m.rating_number,\n",
    "            m.price,\n",
    "            '{category}' AS category\n",
    "        FROM review r\n",
    "        LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "        WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    #save result to parquet, dont forget to delete temp db\n",
    "    con.execute(f\"\"\"\n",
    "        COPY cleaned TO '{output_path}' (FORMAT PARQUET);\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "    print(f\"Saved merged and cleaned data ‚Üí {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f653c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CLEANED_DIR = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\New folder\\raw_files\\cleaned_parquet\"\n",
    "\n",
    "# Only look for categories with review + meta parquet files\n",
    "def get_parquet_categories(cleaned_dir):\n",
    "    review_files = set(\n",
    "        f.replace(\"_review.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_review.parquet\")\n",
    "    )\n",
    "    meta_files = set(\n",
    "        f.replace(\"_meta.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_meta.parquet\")\n",
    "    )\n",
    "    return sorted(review_files & meta_files)\n",
    "\n",
    "categories = get_parquet_categories(CLEANED_DIR)\n",
    "print(\"Found categories:\", categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Home_and_Kitchen', 'Industrial_and_Scientific']\n",
    "for cat in categories:\n",
    "    merge_parquet_to_duckdb(cat, CLEANED_DIR)\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
