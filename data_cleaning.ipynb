{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw_files\")\n",
    "CLEANED_DIR = os.path.join(BASE_DIR, \"cleaned_parquet\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def extract_tar_bz2_files(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar.bz2\"):\n",
    "            filepath = os.path.join(directory, file)\n",
    "            extract_dir = os.path.join(directory, file.replace(\".tar.bz2\", \"\"))\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Extracting: {file}\")\n",
    "            with tarfile.open(filepath, \"r:bz2\") as tar:\n",
    "                tar.extractall(path=extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4458979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: raw_meta_Gift_Cards.tar.bz2\n",
      "Extracting: raw_review_Gift_Cards.tar.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maian\\AppData\\Local\\Temp\\ipykernel_36928\\154044346.py:13: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_dir)\n"
     ]
    }
   ],
   "source": [
    "extract_tar_bz2_files(RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a7dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_and_merge_streaming(category):\n",
    "    review_path = os.path.join(RAW_DIR, f\"raw_review_{category}\", \"data.jsonl\")\n",
    "    meta_path = os.path.join(RAW_DIR, f\"raw_meta_{category}\", \"data.jsonl\")\n",
    "\n",
    "    try:\n",
    "        meta_df = pd.read_json(meta_path, lines=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Meta load failed for {category}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Preprocess metadata\n",
    "    meta_df[\"brand\"] = meta_df[\"details\"].apply(lambda d: d.get(\"brand\") if isinstance(d, dict) else \"Unknown\")\n",
    "    meta_df[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    save_path = os.path.join(CLEANED_DIR, f\"{category}.parquet\")\n",
    "    chunk_iter = pd.read_json(review_path, lines=True, chunksize=100_000)\n",
    "    \n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"[{category}] Processing chunk {i+1}\")\n",
    "\n",
    "        # Filter and clean\n",
    "        chunk = chunk.dropna(subset=[\"rating\", \"text\"])\n",
    "        chunk = chunk[chunk[\"rating\"].between(1, 5)]\n",
    "        chunk = chunk.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "        chunk[\"review_length\"] = chunk[\"text\"].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "        chunk[\"year\"] = pd.to_datetime(chunk[\"timestamp\"], unit='ms', errors='coerce').dt.year\n",
    "\n",
    "        # Merge\n",
    "        merged = pd.merge(chunk, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "        # Append to parquet\n",
    "        if not os.path.exists(save_path):\n",
    "            merged.to_parquet(save_path, index=False)\n",
    "        else:\n",
    "            merged.to_parquet(save_path, index=False, append=True)\n",
    "\n",
    "    print(f\"Saved {category} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2ba484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Gift_Cards...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df712c2be81b4c83af90ecc255ad7cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/153 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a4266ecb094ef699dbc70ca87fca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "def arrow_to_parquet(category):\n",
    "    try:\n",
    "        review_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_review_{category}\"))\n",
    "        meta_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_meta_{category}\"))\n",
    "\n",
    "        # Extract the 'full' split\n",
    "        review_ds = review_dict[\"full\"]\n",
    "        meta_ds = meta_dict[\"full\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {category} â€” failed to load arrow or 'full' split: {e}\")\n",
    "        return\n",
    "\n",
    "    # Save to Parquet\n",
    "    review_path = os.path.join(CLEANED_DIR, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(CLEANED_DIR, f\"{category}_meta.parquet\")\n",
    "    print(f\"Exporting {category}...\")\n",
    "\n",
    "    review_ds.to_parquet(review_path)\n",
    "    meta_ds.to_parquet(meta_path)\n",
    "\n",
    "#Detect categories with both review and meta folders\n",
    "categories = [\n",
    "    name.replace(\"raw_review_\", \"\")\n",
    "    for name in os.listdir(RAW_DIR)\n",
    "    if name.startswith(\"raw_review_\")\n",
    "    and os.path.isdir(os.path.join(RAW_DIR, name))\n",
    "    and os.path.isdir(os.path.join(RAW_DIR, f\"raw_meta_{name.replace('raw_review_', '')}\"))\n",
    "]\n",
    "\n",
    "categories = ['Gift_Cards']\n",
    "\n",
    "for cat in categories:\n",
    "    arrow_to_parquet(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39254134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "    review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "        print(f\"âš ï¸ Skipping {category} â€” one of the parquet files is missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ”„ Merging: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE meta AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "                THEN json_extract(details, '$.brand')::STRING\n",
    "                ELSE 'Unknown'\n",
    "            END AS brand\n",
    "        FROM '{meta_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE cleaned AS\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT\n",
    "                r.user_id,\n",
    "                r.asin,\n",
    "                r.parent_asin,\n",
    "                r.rating,\n",
    "                r.text,\n",
    "                r.verified_purchase,\n",
    "                r.helpful_vote,\n",
    "                array_length(string_split(r.text, ' ')) AS review_length,\n",
    "                strftime(\n",
    "                    CASE \n",
    "                        WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "                        ELSE to_timestamp(CAST(r.timestamp AS DOUBLE) / 1000.0)\n",
    "                    END,\n",
    "                    '%Y'\n",
    "                )::INTEGER AS year,\n",
    "                m.brand,\n",
    "                m.main_category,\n",
    "                m.title,\n",
    "                m.average_rating,\n",
    "                m.rating_number,\n",
    "                m.price,\n",
    "                '{category}' AS category,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY r.user_id, r.asin, r.text\n",
    "                    ORDER BY r.timestamp\n",
    "                ) AS row_num\n",
    "            FROM review r\n",
    "            LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "            WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL\n",
    "        )\n",
    "        WHERE row_num = 1;\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        COPY cleaned TO '{output_path}' (FORMAT PARQUET);\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "    print(f\"Saved merged and cleaned data â†’ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f653c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['Gift_Cards']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Only look for categories with review + meta parquet files\n",
    "def get_parquet_categories(cleaned_dir):\n",
    "    review_files = set(\n",
    "        f.replace(\"_review.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_review.parquet\")\n",
    "    )\n",
    "    meta_files = set(\n",
    "        f.replace(\"_meta.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_meta.parquet\")\n",
    "    )\n",
    "    return sorted(review_files & meta_files)\n",
    "\n",
    "categories = get_parquet_categories(CLEANED_DIR)\n",
    "print(\"Found categories:\", categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04b60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Merging: Gift_Cards\n",
      "Saved merged and cleaned data â†’ C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\Code\\cleaned_parquet\\Gift_Cards_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    merge_parquet_to_duckdb(cat, CLEANED_DIR)\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
