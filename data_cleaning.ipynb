{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de916ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "BASE_DIR = r\"/root\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"Data\")\n",
    "CLEANED_DIR = os.path.join(BASE_DIR, \"cleaned_parquets\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def extract_tar_bz2_files(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar.bz2\"):\n",
    "            filepath = os.path.join(directory, file)\n",
    "            extract_dir = os.path.join(directory, file.replace(\".tar.bz2\", \"\"))\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Extracting: {file}\")\n",
    "            with tarfile.open(filepath, \"r:bz2\") as tar:\n",
    "                tar.extractall(path=extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4458979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: raw_meta_Clothing_Shoes_and_Jewelry.tar.bz2\n",
      "Extracting: raw_review_Clothing_Shoes_and_Jewelry.tar.bz2\n"
     ]
    }
   ],
   "source": [
    "extract_tar_bz2_files(RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_and_merge_streaming(category):\n",
    "    review_path = os.path.join(RAW_DIR, f\"raw_review_{category}\", \"data.jsonl\")\n",
    "    meta_path = os.path.join(RAW_DIR, f\"raw_meta_{category}\", \"data.jsonl\")\n",
    "\n",
    "    try:\n",
    "        meta_df = pd.read_json(meta_path, lines=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Meta load failed for {category}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Preprocess metadata\n",
    "    meta_df[\"brand\"] = meta_df[\"details\"].apply(lambda d: d.get(\"brand\") if isinstance(d, dict) else \"Unknown\")\n",
    "    meta_df[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    save_path = os.path.join(CLEANED_DIR, f\"{category}.parquet\")\n",
    "    chunk_iter = pd.read_json(review_path, lines=True, chunksize=100_000)\n",
    "    \n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"[{category}] Processing chunk {i+1}\")\n",
    "\n",
    "        # Filter and clean\n",
    "        chunk = chunk.dropna(subset=[\"rating\", \"text\"])\n",
    "        chunk = chunk[chunk[\"rating\"].between(1, 5)]\n",
    "        chunk = chunk.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "        chunk[\"review_length\"] = chunk[\"text\"].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "        chunk[\"year\"] = pd.to_datetime(chunk[\"timestamp\"], unit='ms', errors='coerce').dt.year\n",
    "\n",
    "        # Merge\n",
    "        merged = pd.merge(chunk, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "        # Append to parquet\n",
    "        if not os.path.exists(save_path):\n",
    "            merged.to_parquet(save_path, index=False)\n",
    "        else:\n",
    "            merged.to_parquet(save_path, index=False, append=True)\n",
    "\n",
    "    print(f\"Saved {category} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2ba484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64da29eca89443c3ac0b77e7f0bd3744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca78c734914246b6991d8e7305185cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Clothing_Shoes_and_Jewelry...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116d294e1e1041f1b1daf791ebc83735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/66034 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589af5b044f947c2a394cbd401429e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7219 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "def arrow_to_parquet(category):\n",
    "    try:\n",
    "        review_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_review_{category}\", f\"raw_review_{category}\"))\n",
    "        meta_dict = load_from_disk(os.path.join(RAW_DIR, f\"raw_meta_{category}\", f\"raw_meta_{category}\"))\n",
    "\n",
    "        # Extract the 'full' split\n",
    "        review_ds = review_dict[\"full\"]\n",
    "        meta_ds = meta_dict[\"full\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {category} — failed to load arrow or 'full' split: {e}\")\n",
    "        return\n",
    "\n",
    "    # Save to Parquet\n",
    "    review_path = os.path.join(CLEANED_DIR, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(CLEANED_DIR, f\"{category}_meta.parquet\")\n",
    "    print(f\"Exporting {category}...\")\n",
    "\n",
    "    review_ds.to_parquet(review_path)\n",
    "    meta_ds.to_parquet(meta_path)\n",
    "\n",
    "#Detect categories with both review and meta folders\n",
    "categories = [\n",
    "    name.replace(\"raw_review_\", \"\")\n",
    "    for name in os.listdir(RAW_DIR)\n",
    "    if name.startswith(\"raw_review_\")\n",
    "    and os.path.isdir(os.path.join(RAW_DIR, name))\n",
    "    and os.path.isdir(os.path.join(RAW_DIR, f\"raw_meta_{name.replace('raw_review_', '')}\"))\n",
    "]\n",
    "\n",
    "categories = ['Clothing_Shoes_and_Jewelry']\n",
    "\n",
    "for cat in categories:\n",
    "    arrow_to_parquet(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f653c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['Clothing_Shoes_and_Jewelry']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Only look for categories with review + meta parquet files\n",
    "def get_parquet_categories(cleaned_dir):\n",
    "    review_files = set(\n",
    "        f.replace(\"_review.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_review.parquet\")\n",
    "    )\n",
    "    meta_files = set(\n",
    "        f.replace(\"_meta.parquet\", \"\")\n",
    "        for f in os.listdir(cleaned_dir) if f.endswith(\"_meta.parquet\")\n",
    "    )\n",
    "    return sorted(review_files & meta_files)\n",
    "\n",
    "categories = get_parquet_categories(CLEANED_DIR)\n",
    "print(\"Found categories:\", categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac4d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "    review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "        print(f\"⚠ Skipping {category} — one of the parquet files is missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Merging: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "    con.execute(\"PRAGMA max_temp_directory_size='20GiB';\")\n",
    "\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE meta AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "                THEN json_extract(details, '$.brand')::STRING\n",
    "                ELSE 'Unknown'\n",
    "            END AS brand\n",
    "        FROM '{meta_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    # No deduplication\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE cleaned AS\n",
    "        SELECT\n",
    "            r.user_id,\n",
    "            r.asin,\n",
    "            r.parent_asin,\n",
    "            r.rating,\n",
    "            r.text,\n",
    "            r.verified_purchase,\n",
    "            r.helpful_vote,\n",
    "            array_length(string_split(r.text, ' ')) AS review_length,\n",
    "             strftime(\n",
    "                    CASE \n",
    "                        WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "                        ELSE to_timestamp(CAST(r.timestamp AS DOUBLE) / 1000.0)\n",
    "                    END,\n",
    "                    '%Y'\n",
    "                )::INTEGER AS year,\n",
    "            m.brand,\n",
    "            m.main_category,\n",
    "            m.title,\n",
    "            m.average_rating,\n",
    "            m.rating_number,\n",
    "            m.price,\n",
    "            '{category}' AS category\n",
    "        FROM review r\n",
    "        LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "        WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"COPY cleaned TO '{output_path}' (FORMAT PARQUET);\")\n",
    "    con.close()\n",
    "    print(f\"Saved merged and cleaned data → {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c740351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import duckdb\n",
    "\n",
    "def deduplicate_large_parquet_chunked(con, input_path, dedup_columns, order_column='year', chunk_size=250_000):\n",
    "    print(f\"🧠 Using chunked deduplication for large file: {os.path.basename(input_path)}\")\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS temp_all;\")\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE temp_all AS\n",
    "        SELECT * FROM read_parquet('{input_path}', union_by_name=true)\n",
    "        LIMIT 0;\n",
    "    \"\"\")\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        chunk_df = con.execute(f\"\"\"\n",
    "            SELECT * FROM read_parquet('{input_path}', union_by_name=true)\n",
    "            LIMIT {chunk_size} OFFSET {offset}\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        if chunk_df.empty:\n",
    "            break\n",
    "\n",
    "        con.register(\"chunk\", chunk_df)\n",
    "        con.execute(\"INSERT INTO temp_all SELECT * FROM chunk\")\n",
    "        print(f\"✅ Chunk {offset // chunk_size + 1} inserted ({len(chunk_df)} rows)\")\n",
    "        offset += chunk_size\n",
    "\n",
    "    dedup_key = ', '.join(dedup_columns)\n",
    "    con.execute(\"DROP TABLE IF EXISTS deduplicated;\")\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE deduplicated AS\n",
    "        SELECT * EXCLUDE(row_num)\n",
    "        FROM (\n",
    "            SELECT *,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY {dedup_key}\n",
    "                    ORDER BY {order_column}\n",
    "                ) AS row_num\n",
    "            FROM temp_all\n",
    "        )\n",
    "        WHERE row_num = 1;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c92d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_folder_merged_files(input_dir, output_dir, dedup_columns, order_column='year', chunk_threshold_gb=2.0):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_parquets = glob.glob(os.path.join(input_dir, \"*_merged.parquet\"))\n",
    "\n",
    "    print(f\"📁 Found {len(all_parquets)} '_merged' files to deduplicate...\\n\")\n",
    "\n",
    "    for input_path in all_parquets:\n",
    "        filename = os.path.basename(input_path)\n",
    "        name_wo_ext = filename.replace(\"_merged.parquet\", \"\")\n",
    "        output_path = os.path.join(output_dir, f\"{name_wo_ext}_deduped.parquet\")\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"⏩ Skipping (already deduplicated): {filename}\")\n",
    "            continue\n",
    "\n",
    "        file_size_gb = os.path.getsize(input_path) / (1024 ** 3)\n",
    "        con = duckdb.connect()\n",
    "\n",
    "        try:\n",
    "            if file_size_gb >= chunk_threshold_gb:\n",
    "                deduplicate_large_parquet_chunked(\n",
    "                    con,\n",
    "                    input_path,\n",
    "                    dedup_columns=dedup_columns,\n",
    "                    order_column=order_column,\n",
    "                    chunk_size=250_000\n",
    "                )\n",
    "            else:\n",
    "                print(f\"🧼 Deduplicating: {filename} (size: {file_size_gb:.2f} GB)\")\n",
    "                dedup_key = ', '.join(dedup_columns)\n",
    "\n",
    "                con.execute(\"DROP TABLE IF EXISTS deduplicated;\")\n",
    "                con.execute(f\"\"\"\n",
    "                    CREATE TABLE deduplicated AS\n",
    "                    SELECT * EXCLUDE(row_num)\n",
    "                    FROM (\n",
    "                        SELECT *,\n",
    "                            ROW_NUMBER() OVER (\n",
    "                                PARTITION BY {dedup_key}\n",
    "                                ORDER BY {order_column}\n",
    "                            ) AS row_num\n",
    "                        FROM read_parquet('{input_path}', union_by_name=true)\n",
    "                    )\n",
    "                    WHERE row_num = 1;\n",
    "                \"\"\")\n",
    "\n",
    "            con.execute(f\"COPY deduplicated TO '{output_path}' (FORMAT PARQUET);\")\n",
    "            print(f\"✅ Deduplicated and saved to: {output_path}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with {filename}: {e}\\n\")\n",
    "        finally:\n",
    "            con.close()\n",
    "\n",
    "    print(\"🏁 All deduplication complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04b60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: Clothing_Shoes_and_Jewelry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e49e541b138416db8d0d5c52db2adc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a542e0230b425e8b911336f817eb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4670425fdb46bfa827b38ed2a43a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3d797e94374448b60c0b32e7844f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged and cleaned data → /root/cleaned_parquets/Clothing_Shoes_and_Jewelry_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "categories = ['Clothing_Shoes_and_Jewelry']\n",
    "# 'Beauty_and_Personal_Care', 'Electronics', 'Books', 'Clothing_Shoes_and_Jewelry', 'All_Beauty', 'Amazon_Fashion',  'Appliances', 'Arts_Crafts_and_Sewing',  'Baby_Products', 'CDs_and_Vinyl',  'Digital_Music', 'Gift_Cards' 'Cell_Phones_and_Accessories', \n",
    "\n",
    "# deduplicate_folder_merged_files(\n",
    "#     CLEANED_DIR,\n",
    "#     CLEANED_DIR,\n",
    "#     dedup_columns=[\"user_id\", \"asin\", \"text\"],\n",
    "#     order_column=\"year\",\n",
    "#     chunk_threshold_gb=2.0  # Files over 2 GB will trigger chunking\n",
    "# )\n",
    "\n",
    "for cat in categories:\n",
    "    merge_parquet_to_duckdb(cat, CLEANED_DIR)\n",
    "    # deduplicate_all_parquets(CLEANED_DIR)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39254134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "def merge_parquet_to_duckdb(category, cleaned_dir):\n",
    "    review_path = os.path.join(cleaned_dir, f\"{category}_review.parquet\")\n",
    "    meta_path = os.path.join(cleaned_dir, f\"{category}_meta.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(meta_path):\n",
    "        print(f\"⚠️ Skipping {category} — one of the parquet files is missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔄 Merging: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=os.path.join(cleaned_dir, \"temp_duckdb.db\"))\n",
    "    con.execute(\"PRAGMA max_temp_directory_size='64GiB';\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE review AS SELECT * FROM '{review_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE meta AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN try_cast(details AS JSON) IS NOT NULL AND json_extract(details, '$.brand') IS NOT NULL\n",
    "                THEN json_extract(details, '$.brand')::STRING\n",
    "                ELSE 'Unknown'\n",
    "            END AS brand\n",
    "        FROM '{meta_path}';\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE cleaned AS\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT\n",
    "                r.user_id,\n",
    "                r.asin,\n",
    "                r.parent_asin,\n",
    "                r.rating,\n",
    "                r.text,\n",
    "                r.verified_purchase,\n",
    "                r.helpful_vote,\n",
    "                array_length(string_split(r.text, ' ')) AS review_length,\n",
    "                strftime(\n",
    "                    CASE \n",
    "                        WHEN typeof(r.timestamp) = 'VARCHAR' THEN CAST(r.timestamp AS TIMESTAMP)\n",
    "                        ELSE to_timestamp(CAST(r.timestamp AS DOUBLE) / 1000.0)\n",
    "                    END,\n",
    "                    '%Y'\n",
    "                )::INTEGER AS year,\n",
    "                m.brand,\n",
    "                m.main_category,\n",
    "                m.title,\n",
    "                m.average_rating,\n",
    "                m.rating_number,\n",
    "                m.price,\n",
    "                '{category}' AS category,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY r.user_id, r.asin, r.text\n",
    "                    ORDER BY r.timestamp\n",
    "                ) AS row_num\n",
    "            FROM review r\n",
    "            LEFT JOIN meta m ON r.parent_asin = m.parent_asin\n",
    "            WHERE r.rating BETWEEN 1 AND 5 AND r.text IS NOT NULL\n",
    "        )\n",
    "        WHERE row_num = 1;\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        COPY cleaned TO '{output_path}' (FORMAT PARQUET);\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "    print(f\"Saved merged and cleaned data → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def deduplicate_reviews(category, cleaned_dir):\n",
    "    intermediate_path = os.path.join(cleaned_dir, f\"{category}_merged_raw.parquet\")\n",
    "    output_path = os.path.join(cleaned_dir, f\"{category}_merged.parquet\")\n",
    "    temp_db_path = os.path.join(cleaned_dir, \"temp_duckdb.db\")\n",
    "\n",
    "    if not os.path.exists(intermediate_path):\n",
    "        print(f\"⚠️ Skipping {category} — intermediate file missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🧹 Deduplicating in chunks: {category}\")\n",
    "\n",
    "    con = duckdb.connect(database=temp_db_path)\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS temp_reviews;\")\n",
    "    con.execute(f\"CREATE TABLE temp_reviews AS SELECT * FROM read_parquet('{intermediate_path}');\")\n",
    "\n",
    "    hex_chars = '0123456789abcdef'\n",
    "    temp_outputs = []\n",
    "\n",
    "    for char in hex_chars:\n",
    "        print(f\"🧩 Processing chunk for user_id hash prefix '{char}'\")\n",
    "        temp_output_chunk = os.path.join(cleaned_dir, f\"tmp_{category}_{char}.parquet\")\n",
    "        temp_outputs.append(temp_output_chunk)\n",
    "\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (\n",
    "                SELECT *\n",
    "                FROM (\n",
    "                    SELECT *,\n",
    "                        ROW_NUMBER() OVER (\n",
    "                            PARTITION BY user_id, asin, text\n",
    "                            ORDER BY timestamp\n",
    "                        ) AS row_num\n",
    "                    FROM temp_reviews\n",
    "                    WHERE substr(md5(user_id), 1, 1) = '{char}'\n",
    "                )\n",
    "                WHERE row_num = 1\n",
    "            ) TO '{temp_output_chunk}' (FORMAT PARQUET);\n",
    "        \"\"\")\n",
    "\n",
    "    # Merge all chunk outputs\n",
    "    print(f\"🧬 Merging deduplicated chunks for: {category}\")\n",
    "    union_sql = \" UNION ALL \".join([\n",
    "    f\"(SELECT * FROM read_parquet('{p}'))\" for p in temp_outputs])\n",
    "    con.execute(f\"COPY ({union_sql}) TO '{output_path}' (FORMAT PARQUET);\")\n",
    "\n",
    "    # Cleanup\n",
    "    for temp_file in temp_outputs:\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    con.close()\n",
    "    print(f\"✅ Saved deduplicated data → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d96d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import duckdb\n",
    "\n",
    "def deduplicate_all_parquets(folder_path):\n",
    "    parquet_files = glob.glob(os.path.join(folder_path, \"*_merged.parquet\"))\n",
    "    \n",
    "    if not parquet_files:\n",
    "        print(\"No merged parquet files found.\")\n",
    "        return\n",
    "\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    for file_path in parquet_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        output_path = os.path.join(folder_path, filename.replace(\"_merged.parquet\", \"_deduped.parquet\"))\n",
    "        \n",
    "        print(f\"Deduplicating: {filename}\")\n",
    "\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE deduplicated AS\n",
    "            SELECT * EXCLUDE(row_num)\n",
    "            FROM (\n",
    "                SELECT *,\n",
    "                    ROW_NUMBER() OVER (\n",
    "                        PARTITION BY user_id, asin, text\n",
    "                        ORDER BY year\n",
    "                    ) AS row_num\n",
    "                FROM read_parquet('{file_path}')\n",
    "            )\n",
    "            WHERE row_num = 1;\n",
    "        \"\"\")\n",
    "\n",
    "        con.execute(f\"COPY deduplicated TO '{output_path}' (FORMAT PARQUET);\")\n",
    "        print(f\"Saved deduplicated file → {output_path}\")\n",
    "\n",
    "    con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
