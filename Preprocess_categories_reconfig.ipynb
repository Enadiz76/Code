{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556ae6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_path = \"D:\\\\AS3\\\\Raw\"\n",
    "extraction_path = \"D:\\\\AS3\\\\temp\"\n",
    "\n",
    "review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecae9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles the extraction of the brands\n",
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1669661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_dask(category, review_df, meta_df):\n",
    "    output_dir = r\"D:\\AS3\\Cleaned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # print(\"Reading parquet files as Dask DataFrames\")\n",
    "    review_df = dd.from_pandas(review_df, npartitions=4)\n",
    "    meta_df = dd.from_pandas(meta_df, npartitions=4)\n",
    "\n",
    "    print(\"Merging review and meta on 'parent_asin'\")\n",
    "    merged = dd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    if \"rating\" in merged.columns:\n",
    "        merged = merged[merged[\"rating\"].between(1, 5)]\n",
    "    if \"text\" in merged.columns:\n",
    "        merged = merged[merged[\"text\"].notnull() & (merged[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    def safe_extract_brand(row):\n",
    "        try:\n",
    "            return extract_brand(row.get(\"details\"), row.get(\"store\"))\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    print(\"Extracting brand...\")\n",
    "    merged[\"brand\"] = merged.map_partitions(\n",
    "        lambda df: df.apply(lambda row: safe_extract_brand(row), axis=1),\n",
    "        meta=(\"brand\", \"object\")\n",
    "    )\n",
    "    merged[\"brand\"] = merged[\"brand\"].fillna(\"Unknown\")\n",
    "    print(\"Dropping duplicates...\")\n",
    "    merged = merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().map(lambda x: len(x) if x else 0, meta=(\"review_length\", \"int\"))\n",
    "        merged[\"text\"] = merged[\"text\"].str.slice(0, 10000)\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = dd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    necessary_columns = [\n",
    "        \"user_id\", \"asin\", \"parent_asin\", \"rating\", \"text\", \"verified_purchase\",\n",
    "        \"helpful_vote\", \"review_length\", \"year\", \"brand\", \"main_category\",\n",
    "        \"title\", \"average_rating\", \"rating_number\", \"price\"\n",
    "    ]\n",
    "    merged = merged[[col for col in necessary_columns if col in merged.columns]]\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned.parquet\")\n",
    "    merged.to_parquet(output_file, compression=\"snappy\", write_index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "    print(\"Frame shape: \", merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fa6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(category, review_df, meta_df):\n",
    "\n",
    "#     output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     print(\"=== Merging review and meta on 'parent_asin' (LEFT JOIN) ===\")\n",
    "#     merged = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "#     print(f\"After merge: {len(merged)} rows\")\n",
    "\n",
    "#     # invalid ratings\n",
    "#     if \"rating\" in merged.columns:\n",
    "#         before = len(merged)\n",
    "#         merged = merged[merged[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "#         print(f\"After filtering invalid ratings: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "    \n",
    "#     # empty review texts\n",
    "#     if \"text\" in merged.columns:\n",
    "#         before = len(merged)\n",
    "#         merged = merged[merged[\"text\"].notna() & (merged[\"text\"].str.strip() != \"\")]\n",
    "#         print(f\"After dropping empty text: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "#     #  brand\n",
    "#     print(\"Extracting brand from metadata...\")\n",
    "#     merged[\"brand\"] = merged.apply(\n",
    "#         lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1\n",
    "#     )\n",
    "#     merged[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "#     # duplicates\n",
    "#     before = len(merged)\n",
    "#     merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "#     print(f\"After removing duplicates: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "#     # derived columns\n",
    "#     if \"text\" in merged.columns:\n",
    "#         merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "#     if \"timestamp\" in merged.columns:\n",
    "#         merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "#     # cleaned data\n",
    "#     output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "#     merged.to_pickle(output_file, compression=\"bz2\")\n",
    "#     print(f\"Saved cleaned file to {output_file}\")\n",
    "\n",
    "#     return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93548e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_rev_and_meta(extraction_path, category):\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_review_{category}.tar.bz2\")\n",
    "    print(f\"Loading review: {file_path}\")\n",
    "    review_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    review_df = review_dataset[\"full\"].to_pandas()\n",
    "    review_df.to_parquet(os.path.join(extraction_path, f\"{category}_review.parquet\"))\n",
    "\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "    print(f\"Loading meta: {file_path}\")\n",
    "    meta_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    meta_df = meta_dataset[\"full\"].to_pandas()\n",
    "    meta_df.to_parquet(os.path.join(extraction_path, f\"{category}_meta.parquet\"))\n",
    "\n",
    "    return review_df, meta_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362073d",
   "metadata": {},
   "source": [
    "## Define Categories that will be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838366bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\",\n",
    "    \"Pet_Supplies\",\n",
    "    \"Sports_and_Outdoors\",\n",
    "    \"Subscription_Boxes\",\n",
    "    \"Tools_and_Home_Improvement\",\n",
    "    \"Toys_and_Games\",\n",
    "    \"Video_Games\",\n",
    "    \"Unknown\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662850fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading review: D:\\AS3\\Raw\\raw_review_Office_Products.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_review_Office_Products.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_review_Office_Products...\n",
      "Loading meta: D:\\AS3\\Raw\\raw_meta_Office_Products.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_meta_Office_Products.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_meta_Office_Products...\n",
      "Merging review and meta on 'parent_asin'\n",
      "Extracting brand...\n",
      "Dropping duplicates...\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Take operation overflowed binary array capacity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[0;32m      2\u001b[0m     rev_df, meta_df \u001b[38;5;241m=\u001b[39m load_raw_rev_and_meta(extraction_path, category)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mclean_data_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrev_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m, in \u001b[0;36mclean_data_dask\u001b[1;34m(category, review_df, meta_df)\u001b[0m\n\u001b[0;32m     43\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m necessary_columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mcolumns]]\n\u001b[0;32m     45\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cleaned.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[43mmerged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnappy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned data saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, merged\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:5721\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5718\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See dd.to_parquet docstring for more information\"\"\"\u001b[39;00m\n\u001b[0;32m   5719\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 5721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:1057\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m   1054\u001b[0m out \u001b[38;5;241m=\u001b[39m Scalar(graph, final_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m-> 1057\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m fs\u001b[38;5;241m.\u001b[39minvalidate_cache(path)\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\multi.py:289\u001b[0m, in \u001b[0;36mmerge_chunk\u001b[1;34m(lhs, result_meta, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m                 rhs \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{col: right\u001b[38;5;241m.\u001b[39mastype(dtype)})\n\u001b[1;32m--> 289\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# Workaround for pandas bug where if the left frame of a merge operation is\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# empty, the resulting dataframe can have columns in the wrong order.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# https://github.com/pandas-dev/pandas/issues/9937\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lhs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pyarrow\\table.pxi:1079\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.take\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pyarrow\\compute.py:499\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(data, indices, boundscheck, memory_pool)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03mSelect values (or records) from array- or table-like data given integer\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03mselection indices.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m]\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    498\u001b[0m options \u001b[38;5;241m=\u001b[39m TakeOptions(boundscheck\u001b[38;5;241m=\u001b[39mboundscheck)\n\u001b[1;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtake\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pyarrow\\_compute.pyx:598\u001b[0m, in \u001b[0;36mpyarrow._compute.call_function\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pyarrow\\_compute.pyx:393\u001b[0m, in \u001b[0;36mpyarrow._compute.Function.call\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Take operation overflowed binary array capacity"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    rev_df, meta_df = load_raw_rev_and_meta(extraction_path, category)\n",
    "    clean_data_dask(category, rev_df, meta_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
