{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\reviews\" # path to all the raw review files\n",
    "meta_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta\"       # path to all the meta review files\n",
    "\n",
    "review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7a3a",
   "metadata": {},
   "source": [
    "### Load Files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compressed_dataset(compressed_path: Union[str, Path], extract_dir: Optional[Union[str, Path]] = None, cleanup_after_load: bool = True) -> Union[Dataset, DatasetDict]:\n",
    "    \"\"\"\n",
    "    Load a dataset from a compressed archive (tar.gz, tar.bz2, or tar.xz).\n",
    "\n",
    "    Args:\n",
    "        compressed_path: Path to the compressed dataset file\n",
    "        extract_dir: Directory to extract files to (defaults to a temporary directory)\n",
    "        cleanup_after_load: Whether to delete the extracted files after loading\n",
    "                           (only applies to auto-generated temp directories)\n",
    "\n",
    "    Returns:\n",
    "        The loaded dataset (Dataset or DatasetDict)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file doesn't exist or isn't a supported compressed file\n",
    "    \"\"\"\n",
    "    compressed_path = Path(compressed_path)\n",
    "\n",
    "    if not compressed_path.exists():\n",
    "        raise ValueError(f\"File not found: {compressed_path}\")\n",
    "\n",
    "    # check file extension\n",
    "    valid_extensions = [\".tar.gz\", \".tar.bz2\", \".tar.xz\"]\n",
    "    is_valid = False\n",
    "\n",
    "    for ext in valid_extensions:\n",
    "        if compressed_path.name.endswith(ext):\n",
    "            is_valid = True\n",
    "            break\n",
    "\n",
    "    if not is_valid:\n",
    "        raise ValueError(f\"Expected a compressed tar file (.tar.gz, .tar.bz2, or .tar.xz), got: {compressed_path}\")\n",
    "\n",
    "    # get the expected directory name (remove the extension)\n",
    "    dir_name = compressed_path.name\n",
    "    for ext in valid_extensions:\n",
    "        if dir_name.endswith(ext):\n",
    "            dir_name = dir_name[:-len(ext)]\n",
    "            break\n",
    "\n",
    "    # create extraction directory\n",
    "    is_temp_dir = extract_dir is None\n",
    "    if is_temp_dir:\n",
    "        extract_dir = compressed_path.parent / f\"temp_{uuid.uuid4().hex}\"\n",
    "    else:\n",
    "        extract_dir = Path(extract_dir)\n",
    "\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # extract archive\n",
    "        print(f\"Extracting {compressed_path} to {extract_dir}...\")\n",
    "        with tarfile.open(compressed_path, \"r:*\") as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "\n",
    "        # dataset should be in a subdirectory matching the original directory name\n",
    "        dataset_dir = extract_dir / dir_name\n",
    "\n",
    "        if not dataset_dir.exists():\n",
    "            # try to find any directory\n",
    "            extracted_folders = [f for f in extract_dir.iterdir() if f.is_dir()]\n",
    "            if not extracted_folders:\n",
    "                raise ValueError(f\"No folders found in extracted archive: {compressed_path}\")\n",
    "            dataset_dir = extracted_folders[0]\n",
    "            print(f\"Using extracted directory: {dataset_dir}\")\n",
    "\n",
    "        # load dataset\n",
    "        print(f\"Loading dataset from {dataset_dir}...\")\n",
    "        dataset = load_from_disk(str(dataset_dir))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    finally:\n",
    "        # clean up only if it's a temporary directory we created AND cleanup is requested\n",
    "        if cleanup_after_load and is_temp_dir and extract_dir.exists():\n",
    "            print(f\"Cleaning up temporary directory: {extract_dir}\")\n",
    "            shutil.rmtree(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_rev_and_meta(review_path, meta_path):\n",
    "    for file in review_path:\n",
    "        print(f\"{review_path} loading\")\n",
    "        print(f\"{meta_path} loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d2d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles the extraction of the brands\n",
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function merges the review and meta data dataframes, cleans them \n",
    "# and returns the datframe made to ensure that it was put together and contains data\n",
    "def clean_data(category, review_df, meta_df):\n",
    "    output_dir = r\"D:/UWI/Year 3/Sem 2/COMP3610-Big-Data/Assignments/Assignment#3/A3/datasets/output_folder/cleaned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"inner\")\n",
    "    print(\"Merged\")\n",
    "\n",
    "    print(\"Filtering invalid ratings...\")\n",
    "    merged_df = merged_df[merged_df[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "    print(\"Dropping empty review text...\")\n",
    "    merged = merged_df[merged_df[\"text\"].notna() & (merged_df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "    print(\"Removing duplicate reviews...\")\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    print(\"Computing review length...\")\n",
    "    merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    print(\"Extracting year from timestamp...\")\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "    print(\" All cleaning steps completed.\")\n",
    "    \n",
    "    test = merged\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to iterate the categories, clean them and convert to compressed pkl zips\n",
    "# also removes the uncompressed files from the system once they've been done\n",
    "for category in categories:\n",
    "    base_path = r\"\" # replace with path to tar files\n",
    "    meta_path = r\"\" # replace with path to meta pkl files\n",
    "    review_path = r\"\" # replace with path to review pkl files\n",
    "\n",
    "    # review pkled folder\n",
    "    rev_pkl  = r\"/root/Data/output_folder musical-video_games/reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "    meta_pkl = r\"/root/Data/output_folder musical-video_games/meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    \n",
    "    load_compressed_dataset()\n",
    "\n",
    "    review_df = convert_to_df(review_path, category)\n",
    "    meta_df = convert_to_df(meta_path, category)\n",
    "    cleaned = clean_data(category, review_df, meta_df)\n",
    "    print(cleaned)\n",
    "    del cleaned\n",
    "    del meta_df\n",
    "    del review_df\n",
    "\n",
    "    # remove the review and meta pkl files that aren't compressed\n",
    "    if os.path.exists(rev_pkl):\n",
    "        shutil.rmtree(rev_pkl)\n",
    "    else:\n",
    "        print(f\"{rev_pkl} path does not exist\")\n",
    "\n",
    "    if os.path.exists(meta_pkl):\n",
    "        shutil.rmtree(meta_pkl)\n",
    "    else:\n",
    "        print(f\"{meta_pkl} path does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
