{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556ae6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from bigdata_a3_utils import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_path = \"D:\\\\AS3\\\\Raw\"\n",
    "extraction_path = \"D:\\\\AS3\\\\temp\"\n",
    "\n",
    "review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7a3a",
   "metadata": {},
   "source": [
    "### Load Files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe7a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_compressed_dataset(compressed_path , extract_dir = None, cleanup_after_load: bool = True):\n",
    "#     \"\"\"\n",
    "#     Load a dataset from a compressed archive (tar.gz, tar.bz2, or tar.xz).\n",
    "\n",
    "#     Args:\n",
    "#         compressed_path: Path to the compressed dataset file\n",
    "#         extract_dir: Directory to extract files to (defaults to a temporary directory)\n",
    "#         cleanup_after_load: Whether to delete the extracted files after loading\n",
    "#                            (only applies to auto-generated temp directories)\n",
    "\n",
    "#     Returns:\n",
    "#         The loaded dataset (Dataset or DatasetDict)\n",
    "\n",
    "#     Raises:\n",
    "#         ValueError: If the file doesn't exist or isn't a supported compressed file\n",
    "#     \"\"\"\n",
    "#     compressed_path = Path(compressed_path)\n",
    "\n",
    "#     if not compressed_path.exists():\n",
    "#         raise ValueError(f\"File not found: {compressed_path}\")\n",
    "\n",
    "#     # check file extension\n",
    "#     valid_extensions = [\".tar.gz\", \".tar.bz2\", \".tar.xz\"]\n",
    "#     is_valid = False\n",
    "\n",
    "#     for ext in valid_extensions:\n",
    "#         if compressed_path.name.endswith(ext):\n",
    "#             is_valid = True\n",
    "#             break\n",
    "\n",
    "#     if not is_valid:\n",
    "#         raise ValueError(f\"Expected a compressed tar file (.tar.gz, .tar.bz2, or .tar.xz), got: {compressed_path}\")\n",
    "\n",
    "#     # get the expected directory name (remove the extension)\n",
    "#     dir_name = compressed_path.name\n",
    "#     for ext in valid_extensions:\n",
    "#         if dir_name.endswith(ext):\n",
    "#             dir_name = dir_name[:-len(ext)]\n",
    "#             break\n",
    "\n",
    "#     # create extraction directory\n",
    "#     is_temp_dir = extract_dir is None\n",
    "#     if is_temp_dir:\n",
    "#         extract_dir = compressed_path.parent / f\"temp_{uuid.uuid4().hex}\"\n",
    "#     else:\n",
    "#         extract_dir = Path(extract_dir)\n",
    "\n",
    "#     extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     try:\n",
    "#         # extract archive\n",
    "#         print(f\"Extracting {compressed_path} to {extract_dir}...\")\n",
    "#         with tarfile.open(compressed_path, \"r:*\") as tar:\n",
    "#             tar.extractall(path=extract_dir)\n",
    "\n",
    "#         # dataset should be in a subdirectory matching the original directory name\n",
    "#         dataset_dir = extract_dir / dir_name\n",
    "\n",
    "#         if not dataset_dir.exists():\n",
    "#             # try to find any directory\n",
    "#             extracted_folders = [f for f in extract_dir.iterdir() if f.is_dir()]\n",
    "#             if not extracted_folders:\n",
    "#                 raise ValueError(f\"No folders found in extracted archive: {compressed_path}\")\n",
    "#             dataset_dir = extracted_folders[0]\n",
    "#             print(f\"Using extracted directory: {dataset_dir}\")\n",
    "\n",
    "#         # load dataset\n",
    "#         print(f\"Loading dataset from {dataset_dir}...\")\n",
    "#         dataset = load_from_disk(str(dataset_dir))\n",
    "\n",
    "#         return dataset\n",
    "\n",
    "#     finally:\n",
    "#         # clean up only if it's a temporary directory we created AND cleanup is requested\n",
    "#         if cleanup_after_load and is_temp_dir and extract_dir.exists():\n",
    "#             print(f\"Cleaning up temporary directory: {extract_dir}\")\n",
    "#             shutil.rmtree(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecae9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles the extraction of the brands\n",
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95515a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(rev, meta):\n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(\n",
    "        rev,\n",
    "        meta,\n",
    "        left_on=\"parent_asin\",\n",
    "        right_on=\"parent_asin\",\n",
    "        how=\"inner\"\n",
    "        # suffixes=('_reviews', '_meta')\n",
    "        )\n",
    "    print(\"Merged\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function merges the review and meta data dataframes, cleans them \n",
    "# and returns the datframe made to ensure that it was put together and contains data\n",
    "def clean_data(category, merged_df):\n",
    "    output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "\n",
    "    print(\"Filtering invalid ratings...\")\n",
    "    merged_df = merged_df[merged_df[\"rating_x\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "    print(\"Dropping empty review text...\")\n",
    "    merged = merged_df[merged_df[\"text_x\"].notna() & (merged_df[\"text_x\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "    print(\"Removing duplicate reviews...\")\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    print(\"Computing review length...\")\n",
    "    merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    print(\"Extracting year from timestamp...\")\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "    print(\" All cleaning steps completed.\")\n",
    "    \n",
    "    # test = merged\n",
    "    # return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93548e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_rev_and_meta(extraction_path, category):\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_review_{category}.tar.bz2\")\n",
    "    print(file_path)\n",
    "    review_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    review_df = review_dataset[\"full\"].to_pandas() \n",
    "\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "    print(file_path)\n",
    "    meta_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    meta_df = review_dataset[\"full\"].to_pandas()\n",
    "\n",
    "    return review_df, meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662850fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\AS3\\Raw\\raw_review_All_beauty.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_review_All_beauty.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_review_All_beauty...\n",
      "D:\\AS3\\Raw\\raw_meta_All_beauty.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_meta_All_beauty.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_meta_All_beauty...\n",
      "Merging review and meta...\n",
      "Merged\n",
      "          rating_x                                    title_x  \\\n",
      "0              5.0  Such a lovely scent but not overpowering.   \n",
      "1              5.0  Such a lovely scent but not overpowering.   \n",
      "2              5.0  Such a lovely scent but not overpowering.   \n",
      "3              5.0  Such a lovely scent but not overpowering.   \n",
      "4              5.0  Such a lovely scent but not overpowering.   \n",
      "...            ...                                        ...   \n",
      "75797077       5.0                                 Five Stars   \n",
      "75797078       1.0                          Not enough sizes.   \n",
      "75797079       5.0                              Great product   \n",
      "75797080       1.0                       Colors not the same!   \n",
      "75797081       3.0               Nice soap but NOT for \"body\"   \n",
      "\n",
      "                                                     text_x images_x  \\\n",
      "0         This spray is really nice. It smells really go...       []   \n",
      "1         This spray is really nice. It smells really go...       []   \n",
      "2         This spray is really nice. It smells really go...       []   \n",
      "3         This spray is really nice. It smells really go...       []   \n",
      "4         This spray is really nice. It smells really go...       []   \n",
      "...                                                     ...      ...   \n",
      "75797077                                        Love it!!!!       []   \n",
      "75797078  There's literally only 1 choice for each finge...       []   \n",
      "75797079  Highly recommend, if its made in japan, guaran...       []   \n",
      "75797080  These colors are hardly a perfect pair- they a...       []   \n",
      "75797081  This soap smells nice but creates no \"lather\" ...       []   \n",
      "\n",
      "              asin_x parent_asin                     user_id_x    timestamp_x  \\\n",
      "0         B00YQ6X8EO  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ  1588687728923   \n",
      "1         B00YQ6X8EO  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ  1588687728923   \n",
      "2         B00YQ6X8EO  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ  1588687728923   \n",
      "3         B00YQ6X8EO  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ  1588687728923   \n",
      "4         B00YQ6X8EO  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ  1588687728923   \n",
      "...              ...         ...                           ...            ...   \n",
      "75797077  B00IYI1J7I  B00IYI1J7I  AHEZPKZDVADSMOUCJ4QLROTC2Y3A  1409581444000   \n",
      "75797078  B087TPLFS8  B087TPLFS8  AHKWJYF7HVDNQNTM5RBXR4HAH7AQ  1602047774048   \n",
      "75797079  B0BW8D688K  B0BW8D688K  AGJD3TMBPXZPR56YUJVQDRN722DA  1692904042076   \n",
      "75797080  B07F9D8RMB  B07F9D8RMB  AG77BXGO22EA53CSTBD45SQ6XYDQ  1628898254353   \n",
      "75797081  B098QZBNMD  B098QZBNMD  AHHAHXL2NP6VGIHNJ5QYQPKSQYIA  1644033178041   \n",
      "\n",
      "          helpful_vote_x  verified_purchase_x  rating_y  \\\n",
      "0                      0                 True       5.0   \n",
      "1                      0                 True       1.0   \n",
      "2                      0                 True       5.0   \n",
      "3                      0                 True       5.0   \n",
      "4                      0                 True       2.0   \n",
      "...                  ...                  ...       ...   \n",
      "75797077               0                 True       5.0   \n",
      "75797078               0                 True       1.0   \n",
      "75797079               0                 True       5.0   \n",
      "75797080               0                 True       1.0   \n",
      "75797081               0                 True       3.0   \n",
      "\n",
      "                                                    title_y  \\\n",
      "0                 Such a lovely scent but not overpowering.   \n",
      "1                                             Not worth it.   \n",
      "2                                                      love   \n",
      "3                                  Mermaid hair in a bottle   \n",
      "4         It makes my curly/wavy hair way too smooth and...   \n",
      "...                                                     ...   \n",
      "75797077                                         Five Stars   \n",
      "75797078                                  Not enough sizes.   \n",
      "75797079                                      Great product   \n",
      "75797080                               Colors not the same!   \n",
      "75797081                       Nice soap but NOT for \"body\"   \n",
      "\n",
      "                                                     text_y  \\\n",
      "0         This spray is really nice. It smells really go...   \n",
      "1         This does not work as well as other sea mist h...   \n",
      "2         Makes my hair smell amazing and I love the tex...   \n",
      "3         Pros: smells AMAZING like a beachy beverage! (...   \n",
      "4         This product smells divine but it weighs down ...   \n",
      "...                                                     ...   \n",
      "75797077                                        Love it!!!!   \n",
      "75797078  There's literally only 1 choice for each finge...   \n",
      "75797079  Highly recommend, if its made in japan, guaran...   \n",
      "75797080  These colors are hardly a perfect pair- they a...   \n",
      "75797081  This soap smells nice but creates no \"lather\" ...   \n",
      "\n",
      "                                                   images_y      asin_y  \\\n",
      "0                                                        []  B00YQ6X8EO   \n",
      "1                                                        []  B00YQ6X8EO   \n",
      "2                                                        []  B00YQ6X8EO   \n",
      "3         [{'attachment_type': 'IMAGE', 'large_image_url...  B00YQ6X8EO   \n",
      "4                                                        []  B00YQ6X8EO   \n",
      "...                                                     ...         ...   \n",
      "75797077                                                 []  B00IYI1J7I   \n",
      "75797078                                                 []  B087TPLFS8   \n",
      "75797079                                                 []  B0BW8D688K   \n",
      "75797080                                                 []  B07F9D8RMB   \n",
      "75797081                                                 []  B098QZBNMD   \n",
      "\n",
      "                             user_id_y    timestamp_y  helpful_vote_y  \\\n",
      "0         AGKHLEW2SOWHNMFQIJGBECAF7INQ  1588687728923               0   \n",
      "1         AGF2RF5HBCXKEOLTF2LXZTVFYFVQ  1667089254927               0   \n",
      "2         AGPWM36OKQCL3PM4MXZNLP4VVJKA  1455021933000               1   \n",
      "3         AGVVMZD2DJQ7Z3KJJXHFKQHSX7HA  1623855338155               0   \n",
      "4         AE5VLXANR3ZCUZCQAOBJTIEGMZGA  1623342316264               0   \n",
      "...                                ...            ...             ...   \n",
      "75797077  AHEZPKZDVADSMOUCJ4QLROTC2Y3A  1409581444000               0   \n",
      "75797078  AHKWJYF7HVDNQNTM5RBXR4HAH7AQ  1602047774048               0   \n",
      "75797079  AGJD3TMBPXZPR56YUJVQDRN722DA  1692904042076               0   \n",
      "75797080  AG77BXGO22EA53CSTBD45SQ6XYDQ  1628898254353               0   \n",
      "75797081  AHHAHXL2NP6VGIHNJ5QYQPKSQYIA  1644033178041               0   \n",
      "\n",
      "          verified_purchase_y  \n",
      "0                        True  \n",
      "1                        True  \n",
      "2                        True  \n",
      "3                        True  \n",
      "4                        True  \n",
      "...                       ...  \n",
      "75797077                 True  \n",
      "75797078                 True  \n",
      "75797079                 True  \n",
      "75797080                 True  \n",
      "75797081                 True  \n",
      "\n",
      "[75797082 rows x 19 columns]\n",
      "D:\\AS3\\Raw\\raw_review_Appliances.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_review_Appliances.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_review_Appliances...\n",
      "D:\\AS3\\Raw\\raw_meta_Appliances.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_meta_Appliances.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_meta_Appliances...\n",
      "Merging review and meta...\n"
     ]
    }
   ],
   "source": [
    "# Ensure the function iterates over files in the directory and constructs full paths\n",
    "categories = [\"All_beauty\", \"Appliances\"]\n",
    "\n",
    "for category in categories:\n",
    "   rev, meta =  load_raw_rev_and_meta(extraction_path, category)\n",
    "   merged_df = merge_dfs(rev, meta)\n",
    "   print(merged_df)\n",
    "   # clean_data(category, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For loop to iterate the categories, clean them and convert to compressed pkl zips\n",
    "# # also removes the uncompressed files from the system once they've been done\n",
    "# for category in categories:\n",
    "#     base_path = r\"\" # replace with path to tar files\n",
    "#     meta_path = r\"\" # replace with path to meta pkl files\n",
    "#     review_path = r\"\" # replace with path to review pkl files\n",
    "\n",
    "#     # review pkled folder\n",
    "#     rev_pkl  = r\"/root/Data/output_folder musical-video_games/reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "#     meta_pkl = r\"/root/Data/output_folder musical-video_games/meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    \n",
    "#     load_compressed_dataset()\n",
    "\n",
    "#     review_df = convert_to_df(review_path, category)\n",
    "#     meta_df = convert_to_df(meta_path, category)\n",
    "#     cleaned = clean_data(category, review_df, meta_df)\n",
    "#     print(cleaned)\n",
    "#     del cleaned\n",
    "#     del meta_df\n",
    "#     del review_df\n",
    "\n",
    "#     # remove the review and meta pkl files that aren't compressed\n",
    "#     if os.path.exists(rev_pkl):\n",
    "#         shutil.rmtree(rev_pkl)\n",
    "#     else:\n",
    "#         print(f\"{rev_pkl} path does not exist\")\n",
    "\n",
    "#     if os.path.exists(meta_pkl):\n",
    "#         shutil.rmtree(meta_pkl)\n",
    "#     else:\n",
    "#         print(f\"{meta_pkl} path does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
