{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556ae6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from bigdata_a3_utils import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_path = \"D:\\\\AS3\\\\Raw\"\n",
    "extraction_path = \"D:\\\\AS3\\\\temp\"\n",
    "\n",
    "review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7a3a",
   "metadata": {},
   "source": [
    "### Load Files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe7a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_compressed_dataset(compressed_path , extract_dir = None, cleanup_after_load: bool = True):\n",
    "#     \"\"\"\n",
    "#     Load a dataset from a compressed archive (tar.gz, tar.bz2, or tar.xz).\n",
    "\n",
    "#     Args:\n",
    "#         compressed_path: Path to the compressed dataset file\n",
    "#         extract_dir: Directory to extract files to (defaults to a temporary directory)\n",
    "#         cleanup_after_load: Whether to delete the extracted files after loading\n",
    "#                            (only applies to auto-generated temp directories)\n",
    "\n",
    "#     Returns:\n",
    "#         The loaded dataset (Dataset or DatasetDict)\n",
    "\n",
    "#     Raises:\n",
    "#         ValueError: If the file doesn't exist or isn't a supported compressed file\n",
    "#     \"\"\"\n",
    "#     compressed_path = Path(compressed_path)\n",
    "\n",
    "#     if not compressed_path.exists():\n",
    "#         raise ValueError(f\"File not found: {compressed_path}\")\n",
    "\n",
    "#     # check file extension\n",
    "#     valid_extensions = [\".tar.gz\", \".tar.bz2\", \".tar.xz\"]\n",
    "#     is_valid = False\n",
    "\n",
    "#     for ext in valid_extensions:\n",
    "#         if compressed_path.name.endswith(ext):\n",
    "#             is_valid = True\n",
    "#             break\n",
    "\n",
    "#     if not is_valid:\n",
    "#         raise ValueError(f\"Expected a compressed tar file (.tar.gz, .tar.bz2, or .tar.xz), got: {compressed_path}\")\n",
    "\n",
    "#     # get the expected directory name (remove the extension)\n",
    "#     dir_name = compressed_path.name\n",
    "#     for ext in valid_extensions:\n",
    "#         if dir_name.endswith(ext):\n",
    "#             dir_name = dir_name[:-len(ext)]\n",
    "#             break\n",
    "\n",
    "#     # create extraction directory\n",
    "#     is_temp_dir = extract_dir is None\n",
    "#     if is_temp_dir:\n",
    "#         extract_dir = compressed_path.parent / f\"temp_{uuid.uuid4().hex}\"\n",
    "#     else:\n",
    "#         extract_dir = Path(extract_dir)\n",
    "\n",
    "#     extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     try:\n",
    "#         # extract archive\n",
    "#         print(f\"Extracting {compressed_path} to {extract_dir}...\")\n",
    "#         with tarfile.open(compressed_path, \"r:*\") as tar:\n",
    "#             tar.extractall(path=extract_dir)\n",
    "\n",
    "#         # dataset should be in a subdirectory matching the original directory name\n",
    "#         dataset_dir = extract_dir / dir_name\n",
    "\n",
    "#         if not dataset_dir.exists():\n",
    "#             # try to find any directory\n",
    "#             extracted_folders = [f for f in extract_dir.iterdir() if f.is_dir()]\n",
    "#             if not extracted_folders:\n",
    "#                 raise ValueError(f\"No folders found in extracted archive: {compressed_path}\")\n",
    "#             dataset_dir = extracted_folders[0]\n",
    "#             print(f\"Using extracted directory: {dataset_dir}\")\n",
    "\n",
    "#         # load dataset\n",
    "#         print(f\"Loading dataset from {dataset_dir}...\")\n",
    "#         dataset = load_from_disk(str(dataset_dir))\n",
    "\n",
    "#         return dataset\n",
    "\n",
    "#     finally:\n",
    "#         # clean up only if it's a temporary directory we created AND cleanup is requested\n",
    "#         if cleanup_after_load and is_temp_dir and extract_dir.exists():\n",
    "#             print(f\"Cleaning up temporary directory: {extract_dir}\")\n",
    "#             shutil.rmtree(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecae9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles the extraction of the brands\n",
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95515a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(rev, meta, category):\n",
    "    output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(\n",
    "        rev,\n",
    "        meta,\n",
    "        left_on=\"parent_asin\",\n",
    "        right_on=\"parent_asin\",\n",
    "        how=\"inner\"\n",
    "        # suffixes=('_reviews', '_meta')\n",
    "        )\n",
    "    print(\"Merged\")\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged_df.to_pickle(output_file, compression=\"bz2\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e2299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function merges the review and meta data dataframes, cleans them \n",
    "# # and returns the datframe made to ensure that it was put together and contains data\n",
    "# def clean_data(category, merged_df):\n",
    "#     output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "\n",
    "#     print(\"Filtering invalid ratings...\")\n",
    "#     merged_df = merged_df[merged_df[\"rating_x\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "#     print(\"Dropping empty review text...\")\n",
    "#     merged = merged_df[merged_df[\"text_x\"].notna() & (merged_df[\"text_x\"].str.strip() != \"\")]\n",
    "\n",
    "#     print(\"Extracting brand from metadata...\")\n",
    "#     merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "#     print(\"Removing duplicate reviews...\")\n",
    "#     merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text_x\"], keep=\"first\", inplace=True)\n",
    "\n",
    "#     print(\"Computing review length...\")\n",
    "#     merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "#     print(\"Extracting year from timestamp...\")\n",
    "#     merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "#     output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "#     merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "#     print(\" All cleaning steps completed.\")\n",
    "#     print(\" DF shape.\")\n",
    "    \n",
    "#     print(merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc43ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function merges the review and meta data dataframes, cleans them \n",
    "# # and returns the datframe made to ensure that it was put together and contains data\n",
    "# def clean_data(category, merged_df):\n",
    "#     # output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "\n",
    "#     print(\"Filtering invalid ratings...\")\n",
    "#     merged_df = merged_df[merged_df[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "#     print(\"Dropping empty review text...\")\n",
    "#     if \"text\" in merged_df.columns:\n",
    "#         merged_df = merged_df[merged_df[\"text\"].notna() & (merged_df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "#     print(\"Extracting brand from metadata...\")\n",
    "#     if \"brand\" in merged_df.columns:\n",
    "#         merged_df[\"brand\"] = merged_df.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "#     print(\"Removing duplicate reviews...\")\n",
    "#     merged_df.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "#     print(\"Computing review length...\")\n",
    "#     merged_df[\"review_length\"] = merged_df[\"text\"].str.split().apply(len)\n",
    "\n",
    "#     print(\"Extracting year from timestamp...\")\n",
    "#     merged_df[\"year\"] = pd.to_datetime(merged_df[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "#     # output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "#     # merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "#     print(\" All cleaning steps completed.\")\n",
    "#     print(\" DF shape.\")\n",
    "    \n",
    "#     print(merged_df.shape)\n",
    "#     return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(category, review_df, meta_df):\n",
    "\n",
    "    output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=== Merging review and meta on 'parent_asin' (LEFT JOIN) ===\")\n",
    "    merged = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "    print(f\"After merge: {len(merged)} rows\")\n",
    "\n",
    "    # invalid ratings\n",
    "    if \"rating\" in merged.columns:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "        print(f\"After filtering invalid ratings: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "    \n",
    "    # empty review texts\n",
    "    if \"text\" in merged.columns:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"text\"].notna() & (merged[\"text\"].str.strip() != \"\")]\n",
    "        print(f\"After dropping empty text: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "    #  brand\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(\n",
    "        lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1\n",
    "    )\n",
    "    merged[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    # duplicates\n",
    "    before = len(merged)\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "    print(f\"After removing duplicates: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "    # derived columns\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    # cleaned data\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "    print(f\"Saved cleaned file to {output_file}\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93548e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_rev_and_meta(extraction_path, category):\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_review_{category}.tar.bz2\")\n",
    "    print(file_path)\n",
    "    review_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    review_df = review_dataset[\"full\"].to_pandas() \n",
    "\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "    print(file_path)\n",
    "    meta_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    meta_df = review_dataset[\"full\"].to_pandas()\n",
    "\n",
    "    return review_df, meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662850fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\AS3\\Raw\\raw_review_All_Beauty.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_review_All_Beauty.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_review_All_Beauty...\n",
      "D:\\AS3\\Raw\\raw_meta_All_Beauty.tar.bz2\n",
      "Extracting D:\\AS3\\Raw\\raw_meta_All_Beauty.tar.bz2 to D:\\AS3\\temp...\n",
      "Loading dataset from D:\\AS3\\temp\\raw_meta_All_Beauty...\n",
      "=== Merging review and meta on 'parent_asin' (LEFT JOIN) ===\n",
      "After merge: 75797082 rows\n",
      "Extracting brand from metadata...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Index(['user_id', 'asin', 'text'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19040\\426450993.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m    \u001b[0mrev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mload_raw_rev_and_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextraction_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m    \u001b[1;31m# merged_df = merge_dfs(rev, meta)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m    \u001b[1;31m# print(merged_df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m    \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19040\\1750748675.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(category, review_df, meta_df)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmerged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brand\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mbefore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mmerged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"user_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"asin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"first\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"After removing duplicates: {len(merged)} rows (dropped {before - len(merged)})\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# derived columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6563\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6564\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6566\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6568\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6694\u001b[0m         \u001b[1;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6695\u001b[0m         \u001b[1;31m# key that doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6696\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6698\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6701\u001b[0m             \u001b[1;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['user_id', 'asin', 'text'], dtype='object')"
     ]
    }
   ],
   "source": [
    "# Ensure the function iterates over files in the directory and constructs full paths\n",
    "categories = [\"All_beauty\", \"Appliances\"]\n",
    "# categories = [\"All_Beauty\"]\n",
    "\n",
    "for category in categories:\n",
    "   rev, meta =  load_raw_rev_and_meta(extraction_path, category)\n",
    "   # merged_df = merge_dfs(rev, meta)\n",
    "   # print(merged_df)\n",
    "   clean_data(category, rev, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For loop to iterate the categories, clean them and convert to compressed pkl zips\n",
    "# # also removes the uncompressed files from the system once they've been done\n",
    "# for category in categories:\n",
    "#     base_path = r\"\" # replace with path to tar files\n",
    "#     meta_path = r\"\" # replace with path to meta pkl files\n",
    "#     review_path = r\"\" # replace with path to review pkl files\n",
    "\n",
    "#     # review pkled folder\n",
    "#     rev_pkl  = r\"/root/Data/output_folder musical-video_games/reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "#     meta_pkl = r\"/root/Data/output_folder musical-video_games/meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    \n",
    "#     load_compressed_dataset()\n",
    "\n",
    "#     review_df = convert_to_df(review_path, category)\n",
    "#     meta_df = convert_to_df(meta_path, category)\n",
    "#     cleaned = clean_data(category, review_df, meta_df)\n",
    "#     print(cleaned)\n",
    "#     del cleaned\n",
    "#     del meta_df\n",
    "#     del review_df\n",
    "\n",
    "#     # remove the review and meta pkl files that aren't compressed\n",
    "#     if os.path.exists(rev_pkl):\n",
    "#         shutil.rmtree(rev_pkl)\n",
    "#     else:\n",
    "#         print(f\"{rev_pkl} path does not exist\")\n",
    "\n",
    "#     if os.path.exists(meta_pkl):\n",
    "#         shutil.rmtree(meta_pkl)\n",
    "#     else:\n",
    "#         print(f\"{meta_pkl} path does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tarfile\n",
    "# import json\n",
    "\n",
    "# def extract_tar_bz2(tar_path, extract_to):\n",
    "#     with tarfile.open(tar_path, 'r:bz2') as tar:\n",
    "#         tar.extractall(path=extract_to)\n",
    "\n",
    "# def process_jsonl_file(file_path, output_path, clean_fn=None):\n",
    "#     with open(file_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "#         for line in infile:\n",
    "#             try:\n",
    "#                 obj = json.loads(line)\n",
    "#                 if clean_fn:\n",
    "#                     obj = clean_fn(obj)\n",
    "#                     if obj is None:\n",
    "#                         continue\n",
    "#                 outfile.write(json.dumps(obj) + '\\n')\n",
    "#             except json.JSONDecodeError:\n",
    "#                 continue\n",
    "\n",
    "# def basic_clean_review(obj):\n",
    "#     # Example: skip entries without reviewText\n",
    "#     if not obj.get(\"reviewText\"):\n",
    "#         return None\n",
    "#     return obj\n",
    "\n",
    "# def basic_clean_meta(obj):\n",
    "#     # Example: skip entries with no title\n",
    "#     if not obj.get(\"title\"):\n",
    "#         return None\n",
    "#     return obj\n",
    "\n",
    "# def process_category(extraction_path, category):\n",
    "#     rev_tar = os.path.join(raw_files_path, f\"raw_review_{category}.tar.bz2\")\n",
    "#     meta_tar = os.path.join(raw_files_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "#     rev_dir = os.path.join(extraction_path, f\"{category}_reviews\")\n",
    "#     meta_dir = os.path.join(extraction_path, f\"{category}_meta\")\n",
    "#     os.makedirs(rev_dir, exist_ok=True)\n",
    "#     os.makedirs(meta_dir, exist_ok=True)\n",
    "\n",
    "#     extract_tar_bz2(rev_tar, rev_dir)\n",
    "#     extract_tar_bz2(meta_tar, meta_dir)\n",
    "\n",
    "#     # Assume file inside tar is named {category}.json.gz or json\n",
    "#     review_path = [f for f in os.listdir(rev_dir) if f.endswith('.json') or f.endswith('.jsonl')][0]\n",
    "#     meta_path = [f for f in os.listdir(meta_dir) if f.endswith('.json') or f.endswith('.jsonl')][0]\n",
    "\n",
    "#     cleaned_review_out = os.path.join(extraction_path, f\"{category}_cleaned_reviews.jsonl\")\n",
    "#     cleaned_meta_out = os.path.join(extraction_path, f\"{category}_cleaned_meta.jsonl\")\n",
    "\n",
    "#     process_jsonl_file(os.path.join(rev_dir, review_path), cleaned_review_out, clean_fn=basic_clean_review)\n",
    "#     process_jsonl_file(os.path.join(meta_dir, meta_path), cleaned_meta_out, clean_fn=basic_clean_meta)\n",
    "\n",
    "#     print(f\"✅ Cleaned files saved for {category}\")\n",
    "\n",
    "# # Example usage\n",
    "# for category in [\"All_beauty\", \"Appliances\"]:\n",
    "#     process_category(extraction_path, category)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
