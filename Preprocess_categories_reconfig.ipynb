{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\reviews\" # path to all the raw review files\n",
    "meta_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta\"       # path to all the meta review files\n",
    "\n",
    "review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7b173",
   "metadata": {},
   "source": [
    "### Extract Arrow Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the arrow files from the tar paths temporarily it is run in the preprocess category function\n",
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd657d03",
   "metadata": {},
   "source": [
    "### Put together files to be placed into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this function is used to extract all the arrow files and break them down to even smaller\n",
    "sizes if they pass a certain size limit. it then puts them together into tables and pkl file batches\n",
    "and deletes the temp folder of arrow files\n",
    "'''\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category,batch_size=1000):\n",
    "    temp_path = \"Data/temp_extract\" # change as needed\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"Extracting tar files...\")\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "\n",
    "            pkl_output_path = os.path.join(output_folder, f\"{folder_name}_pkl\")\n",
    "            os.makedirs(pkl_output_path, exist_ok=True)\n",
    "\n",
    "            # print(f\"Streaming {arrow_file.name} â†’ {parquet_output_path}\")\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "            batch_num = 0\n",
    "\n",
    "            for i, row in enumerate(dataset):\n",
    "                if not row:\n",
    "                    continue\n",
    "\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    table = pa.Table.from_pylist(batch)\n",
    "                    # pq.write_to_dataset(table, root_path=parquet_output_path)\n",
    "\n",
    "                    # convert to pandas and save as .pkl batch\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_pickle(os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\"))\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows) to .pkl\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "\n",
    "            # Final batch\n",
    "            if batch:\n",
    "                table = pa.Table.from_pylist(batch)\n",
    "                # pq.write_to_dataset(table, root_path=parquet_output_path)\n",
    "\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_pickle(os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\"))\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    shutil.rmtree(temp_path)\n",
    "    print(\"All done, temp folder removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f4ba1",
   "metadata": {},
   "source": [
    "### Convert files to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the pkl files to dataframes by taking in the pkl directories\n",
    "# feed the review pkl path with the category to here followed by the meta with the category\n",
    "# there is 2 separate calls to this function in the for loop below\n",
    "# the data frames returned need to be stored to be merged and then cleaned\n",
    "def convert_to_df(folder, category):\n",
    "    df_r = []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith(\".pkl\") and category.lower() in fname.lower():\n",
    "            try:\n",
    "                file_path = os.path.join(folder, fname)\n",
    "                review_df = pd.read_pickle(file_path)\n",
    "                print(f\"{fname} loaded: shape = {review_df.shape}\")\n",
    "                df_r.append(review_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {fname}:\", e)\n",
    "\n",
    "    if df_r:\n",
    "        review_df = pd.concat(df_r, ignore_index=True)\n",
    "        print(\"All .pkl files loaded. Final shape:\", review_df.shape)\n",
    "\n",
    "    print(\"Removed reviews pkl folder\")\n",
    "    return review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function merges the review and meta data dataframes, cleans them \n",
    "# and returns the datframe made to ensure that it was put together and contains data\n",
    "def clean_data(category, review_df, meta_df):\n",
    "    output_dir = r\"D:/UWI/Year 3/Sem 2/COMP3610-Big-Data/Assignments/Assignment#3/A3/datasets/output_folder/cleaned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"inner\")\n",
    "    print(\"Merged\")\n",
    "\n",
    "    print(\"Filtering invalid ratings...\")\n",
    "    merged_df = merged_df[merged_df[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "    print(\"Dropping empty review text...\")\n",
    "    merged = merged_df[merged_df[\"text\"].notna() & (merged_df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "    print(\"Removing duplicate reviews...\")\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    print(\"Computing review length...\")\n",
    "    merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    print(\"Extracting year from timestamp...\")\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "    print(\" All cleaning steps completed.\")\n",
    "    \n",
    "    test = merged\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to iterate the categories, clean them and convert to compressed pkl zips\n",
    "# also removes the uncompressed files from the system once they've been done\n",
    "for category in categories:\n",
    "    base_path = r\"\" # replace with path to tar files\n",
    "    meta_path = r\"\" # replace with path to meta pkl files\n",
    "    review_path = r\"\" # replace with path to review pkl files\n",
    "\n",
    "    # review pkled folder\n",
    "    rev_pkl  = r\"/root/Data/output_folder musical-video_games/reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "    meta_pkl = r\"/root/Data/output_folder musical-video_games/meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    # preprocess_category(meta_path, review_path, \"output_folder\", category)\n",
    "    review_df = convert_to_df(review_path, category)\n",
    "    meta_df = convert_to_df(meta_path, category)\n",
    "    cleaned = clean_data(category, review_df, meta_df)\n",
    "    print(cleaned)\n",
    "    del cleaned\n",
    "    del meta_df\n",
    "    del review_df\n",
    "\n",
    "    # remove the review and meta pkl files that aren't compressed\n",
    "    if os.path.exists(rev_pkl):\n",
    "        shutil.rmtree(rev_pkl)\n",
    "    else:\n",
    "        print(f\"{rev_pkl} path does not exist\")\n",
    "\n",
    "    if os.path.exists(meta_pkl):\n",
    "        shutil.rmtree(meta_pkl)\n",
    "    else:\n",
    "        print(f\"{meta_pkl} path does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
