{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ae6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_path = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\raw_files\"\n",
    "extraction_path = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\output_folder\\temp_extract\"\n",
    "\n",
    "# review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "# meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles the extraction of the brands\n",
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_dask(category, review_df, meta_df):\n",
    "    output_dir = r\"C:\\Users\\maian\\Downloads\\cleaned files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # print(\"Reading parquet files as Dask DataFrames\")\n",
    "    review_df = dd.from_pandas(review_df, npartitions=4)\n",
    "    meta_df = dd.from_pandas(meta_df, npartitions=4)\n",
    "\n",
    "    print(\"Merging review and meta on 'parent_asin'\")\n",
    "    merged = dd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    if \"rating\" in merged.columns:\n",
    "        merged = merged[merged[\"rating\"].between(1, 5)]\n",
    "    if \"text\" in merged.columns:\n",
    "        merged = merged[merged[\"text\"].notnull() & (merged[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    def safe_extract_brand(row):\n",
    "        try:\n",
    "            return extract_brand(row.get(\"details\"), row.get(\"store\"))\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    print(\"Extracting brand...\")\n",
    "    merged[\"brand\"] = merged.map_partitions(\n",
    "        lambda df: df.apply(lambda row: safe_extract_brand(row), axis=1),\n",
    "        meta=(\"brand\", \"object\")\n",
    "    )\n",
    "    merged[\"brand\"] = merged[\"brand\"].fillna(\"Unknown\")\n",
    "    print(\"Dropping duplicates...\")\n",
    "    merged = merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().map(lambda x: len(x) if x else 0, meta=(\"review_length\", \"int\"))\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = dd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    necessary_columns = [\n",
    "        \"user_id\", \"asin\", \"parent_asin\", \"rating\", \"text\", \"verified_purchase\",\n",
    "        \"helpful_vote\", \"review_length\", \"year\", \"brand\", \"main_category\",\n",
    "        \"title\", \"average_rating\", \"rating_number\", \"price\"\n",
    "    ]\n",
    "    merged = merged[[col for col in necessary_columns if col in merged.columns]]\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned.parquet\")\n",
    "    merged.to_parquet(output_file, compression=\"snappy\", write_index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05177410",
   "metadata": {},
   "source": [
    "## Define Categories that will be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health_and_Household\",\n",
    "    \"Home_and_Kitchen\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(category, review_df, meta_df):\n",
    "\n",
    "#     output_dir = r\"D:\\AS3\\pkl_dir\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     print(\"=== Merging review and meta on 'parent_asin' (LEFT JOIN) ===\")\n",
    "#     merged = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "#     print(f\"After merge: {len(merged)} rows\")\n",
    "\n",
    "#     # invalid ratings\n",
    "#     if \"rating\" in merged.columns:\n",
    "#         before = len(merged)\n",
    "#         merged = merged[merged[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "#         print(f\"After filtering invalid ratings: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "    \n",
    "#     # empty review texts\n",
    "#     if \"text\" in merged.columns:\n",
    "#         before = len(merged)\n",
    "#         merged = merged[merged[\"text\"].notna() & (merged[\"text\"].str.strip() != \"\")]\n",
    "#         print(f\"After dropping empty text: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "#     #  brand\n",
    "#     print(\"Extracting brand from metadata...\")\n",
    "#     merged[\"brand\"] = merged.apply(\n",
    "#         lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1\n",
    "#     )\n",
    "#     merged[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "#     # duplicates\n",
    "#     before = len(merged)\n",
    "#     merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "#     print(f\"After removing duplicates: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "#     # derived columns\n",
    "#     if \"text\" in merged.columns:\n",
    "#         merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "#     if \"timestamp\" in merged.columns:\n",
    "#         merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "#     # cleaned data\n",
    "#     output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "#     merged.to_pickle(output_file, compression=\"bz2\")\n",
    "#     print(f\"Saved cleaned file to {output_file}\")\n",
    "\n",
    "#     return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93548e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_rev_and_meta(extraction_path, category):\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_review_{category}.tar.bz2\")\n",
    "    print(f\"Loading review: {file_path}\")\n",
    "    review_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    review_df = review_dataset[\"full\"].to_pandas()\n",
    "    review_df.to_parquet(os.path.join(extraction_path, f\"{category}_review.parquet\"))\n",
    "\n",
    "    file_path = os.path.join(raw_files_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "    print(f\"Loading meta: {file_path}\")\n",
    "    meta_dataset = load_compressed_dataset(file_path, extraction_path)\n",
    "    meta_df = meta_dataset[\"full\"].to_pandas()\n",
    "    meta_df.to_parquet(os.path.join(extraction_path, f\"{category}_meta.parquet\"))\n",
    "\n",
    "    return review_df, meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662850fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\"]\n",
    "\n",
    "for category in categories:\n",
    "    rev_df, meta_df = load_raw_rev_and_meta(extraction_path, category)\n",
    "    clean_data_dask(category, rev_df, meta_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For loop to iterate the categories, clean them and convert to compressed pkl zips\n",
    "# # also removes the uncompressed files from the system once they've been done\n",
    "# for category in categories:\n",
    "#     base_path = r\"\" # replace with path to tar files\n",
    "#     meta_path = r\"\" # replace with path to meta pkl files\n",
    "#     review_path = r\"\" # replace with path to review pkl files\n",
    "\n",
    "#     # review pkled folder\n",
    "#     rev_pkl  = r\"/root/Data/output_folder musical-video_games/reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "#     meta_pkl = r\"/root/Data/output_folder musical-video_games/meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    \n",
    "#     load_compressed_dataset()\n",
    "\n",
    "#     review_df = convert_to_df(review_path, category)\n",
    "#     meta_df = convert_to_df(meta_path, category)\n",
    "#     cleaned = clean_data(category, review_df, meta_df)\n",
    "#     print(cleaned)\n",
    "#     del cleaned\n",
    "#     del meta_df\n",
    "#     del review_df\n",
    "\n",
    "#     # remove the review and meta pkl files that aren't compressed\n",
    "#     if os.path.exists(rev_pkl):\n",
    "#         shutil.rmtree(rev_pkl)\n",
    "#     else:\n",
    "#         print(f\"{rev_pkl} path does not exist\")\n",
    "\n",
    "#     if os.path.exists(meta_pkl):\n",
    "#         shutil.rmtree(meta_pkl)\n",
    "#     else:\n",
    "#         print(f\"{meta_pkl} path does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tarfile\n",
    "# import json\n",
    "\n",
    "# def extract_tar_bz2(tar_path, extract_to):\n",
    "#     with tarfile.open(tar_path, 'r:bz2') as tar:\n",
    "#         tar.extractall(path=extract_to)\n",
    "\n",
    "# def process_jsonl_file(file_path, output_path, clean_fn=None):\n",
    "#     with open(file_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "#         for line in infile:\n",
    "#             try:\n",
    "#                 obj = json.loads(line)\n",
    "#                 if clean_fn:\n",
    "#                     obj = clean_fn(obj)\n",
    "#                     if obj is None:\n",
    "#                         continue\n",
    "#                 outfile.write(json.dumps(obj) + '\\n')\n",
    "#             except json.JSONDecodeError:\n",
    "#                 continue\n",
    "\n",
    "# def basic_clean_review(obj):\n",
    "#     # Example: skip entries without reviewText\n",
    "#     if not obj.get(\"reviewText\"):\n",
    "#         return None\n",
    "#     return obj\n",
    "\n",
    "# def basic_clean_meta(obj):\n",
    "#     # Example: skip entries with no title\n",
    "#     if not obj.get(\"title\"):\n",
    "#         return None\n",
    "#     return obj\n",
    "\n",
    "# def process_category(extraction_path, category):\n",
    "#     rev_tar = os.path.join(raw_files_path, f\"raw_review_{category}.tar.bz2\")\n",
    "#     meta_tar = os.path.join(raw_files_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "#     rev_dir = os.path.join(extraction_path, f\"{category}_reviews\")\n",
    "#     meta_dir = os.path.join(extraction_path, f\"{category}_meta\")\n",
    "#     os.makedirs(rev_dir, exist_ok=True)\n",
    "#     os.makedirs(meta_dir, exist_ok=True)\n",
    "\n",
    "#     extract_tar_bz2(rev_tar, rev_dir)\n",
    "#     extract_tar_bz2(meta_tar, meta_dir)\n",
    "\n",
    "#     # Assume file inside tar is named {category}.json.gz or json\n",
    "#     review_path = [f for f in os.listdir(rev_dir) if f.endswith('.json') or f.endswith('.jsonl')][0]\n",
    "#     meta_path = [f for f in os.listdir(meta_dir) if f.endswith('.json') or f.endswith('.jsonl')][0]\n",
    "\n",
    "#     cleaned_review_out = os.path.join(extraction_path, f\"{category}_cleaned_reviews.jsonl\")\n",
    "#     cleaned_meta_out = os.path.join(extraction_path, f\"{category}_cleaned_meta.jsonl\")\n",
    "\n",
    "#     process_jsonl_file(os.path.join(rev_dir, review_path), cleaned_review_out, clean_fn=basic_clean_review)\n",
    "#     process_jsonl_file(os.path.join(meta_dir, meta_path), cleaned_meta_out, clean_fn=basic_clean_meta)\n",
    "\n",
    "#     print(f\"✅ Cleaned files saved for {category}\")\n",
    "\n",
    "# # Example usage\n",
    "# for category in [\"All_beauty\", \"Appliances\"]:\n",
    "#     process_category(extraction_path, category)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
