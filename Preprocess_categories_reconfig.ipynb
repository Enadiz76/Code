{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36343c78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556ae6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bad0d",
   "metadata": {},
   "source": [
    "### Files/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9264b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_path = \"D:\\\\AS3\\\\Compressed_Files\"\n",
    "extraction_path = \"D:\\\\AS3\\\\Compressed_Files\"\n",
    "\n",
    "review_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\review_pkl\" #path of the review pkl files\n",
    "meta_pkls_path = \"C:\\\\Big Data\\\\A3\\\\Data\\\\meta_pkl\"     #path of the meta pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187605c2",
   "metadata": {},
   "source": [
    "### Categories to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unknown', 'Magazine_Subscriptions', 'Movies_and_TV', \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewellery\", \"Digital_Music\", \"Hanmade_Products\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Electronics\"] # These are the ones that we have left to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7a3a",
   "metadata": {},
   "source": [
    "### Load Files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe7a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compressed_dataset(compressed_path , extract_dir = None, cleanup_after_load: bool = True):\n",
    "    \"\"\"\n",
    "    Load a dataset from a compressed archive (tar.gz, tar.bz2, or tar.xz).\n",
    "\n",
    "    Args:\n",
    "        compressed_path: Path to the compressed dataset file\n",
    "        extract_dir: Directory to extract files to (defaults to a temporary directory)\n",
    "        cleanup_after_load: Whether to delete the extracted files after loading\n",
    "                           (only applies to auto-generated temp directories)\n",
    "\n",
    "    Returns:\n",
    "        The loaded dataset (Dataset or DatasetDict)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file doesn't exist or isn't a supported compressed file\n",
    "    \"\"\"\n",
    "    compressed_path = Path(compressed_path)\n",
    "\n",
    "    if not compressed_path.exists():\n",
    "        raise ValueError(f\"File not found: {compressed_path}\")\n",
    "\n",
    "    # check file extension\n",
    "    valid_extensions = [\".tar.gz\", \".tar.bz2\", \".tar.xz\"]\n",
    "    is_valid = False\n",
    "\n",
    "    for ext in valid_extensions:\n",
    "        if compressed_path.name.endswith(ext):\n",
    "            is_valid = True\n",
    "            break\n",
    "\n",
    "    if not is_valid:\n",
    "        raise ValueError(f\"Expected a compressed tar file (.tar.gz, .tar.bz2, or .tar.xz), got: {compressed_path}\")\n",
    "\n",
    "    # get the expected directory name (remove the extension)\n",
    "    dir_name = compressed_path.name\n",
    "    for ext in valid_extensions:\n",
    "        if dir_name.endswith(ext):\n",
    "            dir_name = dir_name[:-len(ext)]\n",
    "            break\n",
    "\n",
    "    # create extraction directory\n",
    "    is_temp_dir = extract_dir is None\n",
    "    if is_temp_dir:\n",
    "        extract_dir = compressed_path.parent / f\"temp_{uuid.uuid4().hex}\"\n",
    "    else:\n",
    "        extract_dir = Path(extract_dir)\n",
    "\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # extract archive\n",
    "        print(f\"Extracting {compressed_path} to {extract_dir}...\")\n",
    "        with tarfile.open(compressed_path, \"r:*\") as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "\n",
    "        # dataset should be in a subdirectory matching the original directory name\n",
    "        dataset_dir = extract_dir / dir_name\n",
    "\n",
    "        if not dataset_dir.exists():\n",
    "            # try to find any directory\n",
    "            extracted_folders = [f for f in extract_dir.iterdir() if f.is_dir()]\n",
    "            if not extracted_folders:\n",
    "                raise ValueError(f\"No folders found in extracted archive: {compressed_path}\")\n",
    "            dataset_dir = extracted_folders[0]\n",
    "            print(f\"Using extracted directory: {dataset_dir}\")\n",
    "\n",
    "        # load dataset\n",
    "        print(f\"Loading dataset from {dataset_dir}...\")\n",
    "        dataset = load_from_disk(str(dataset_dir))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    finally:\n",
    "        # clean up only if it's a temporary directory we created AND cleanup is requested\n",
    "        if cleanup_after_load and is_temp_dir and extract_dir.exists():\n",
    "            print(f\"Cleaning up temporary directory: {extract_dir}\")\n",
    "            shutil.rmtree(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecae9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles the extraction of the brands\n",
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2554eb",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e2299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function merges the review and meta data dataframes, cleans them \n",
    "# and returns the datframe made to ensure that it was put together and contains data\n",
    "def clean_data(category, review_df, meta_df):\n",
    "    output_dir = r\"D:/UWI/Year 3/Sem 2/COMP3610-Big-Data/Assignments/Assignment#3/A3/datasets/output_folder/cleaned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"inner\")\n",
    "    print(\"Merged\")\n",
    "\n",
    "    print(\"Filtering invalid ratings...\")\n",
    "    merged_df = merged_df[merged_df[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "    print(\"Dropping empty review text...\")\n",
    "    merged = merged_df[merged_df[\"text\"].notna() & (merged_df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "    print(\"Removing duplicate reviews...\")\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    print(\"Computing review length...\")\n",
    "    merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    print(\"Extracting year from timestamp...\")\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "    print(\" All cleaning steps completed.\")\n",
    "    \n",
    "    test = merged\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6dcb0",
   "metadata": {},
   "source": [
    "### Process Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93548e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_rev_and_meta(extraction_path, category):\n",
    "        for file_name in os.listdir(raw_files_path):\n",
    "            if \"review\" in raw_files_path:\n",
    "                raw_name = f\"raw_review_{category}.tar.bz2\" \n",
    "                file_path = os.path.join(raw_files_path, raw_name)\n",
    "                print(f\"Handling {file_path}\")\n",
    "                review_dataset = load_compressed_dataset(file_path)\n",
    "                review_df = review_dataset.to_pandas()\n",
    "            else:\n",
    "                raw_name = f\"raw_review_{category}.tar.bz2\" \n",
    "                file_path = os.path.join(raw_files_path, raw_name)\n",
    "                print(f\"Handling {file_path}\")\n",
    "                meta_dataset = load_compressed_dataset(file_path)\n",
    "                meta_df = meta_dataset.to_pandas()           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "662850fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling D:\\AS3\\Compressed_Files\\raw_review_All_beauty.tar.bz2\n",
      "Extracting D:\\AS3\\Compressed_Files\\raw_review_All_beauty.tar.bz2 to D:\\AS3\\Compressed_Files\\temp_4b38c844ee9c46f5b73ce28bb8f08da7...\n",
      "Loading dataset from D:\\AS3\\Compressed_Files\\temp_4b38c844ee9c46f5b73ce28bb8f08da7\\raw_review_All_beauty...\n",
      "Cleaning up temporary directory: D:\\AS3\\Compressed_Files\\temp_4b38c844ee9c46f5b73ce28bb8f08da7\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'D:\\\\AS3\\\\Compressed_Files\\\\temp_4b38c844ee9c46f5b73ce28bb8f08da7\\\\raw_review_All_Beauty\\\\full\\\\data-00000-of-00001.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll_beauty\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppliances\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mload_raw_rev_and_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextraction_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# load_raw_rev_and_meta(extraction_path, paths)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mload_raw_rev_and_meta\u001b[1;34m(extraction_path, category)\u001b[0m\n\u001b[0;32m     11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_files_path, raw_name)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHandling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m meta_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_compressed_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m meta_df \u001b[38;5;241m=\u001b[39m meta_dataset\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "Cell \u001b[1;32mIn[15], line 77\u001b[0m, in \u001b[0;36mload_compressed_dataset\u001b[1;34m(compressed_path, extract_dir, cleanup_after_load)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup_after_load \u001b[38;5;129;01mand\u001b[39;00m is_temp_dir \u001b[38;5;129;01mand\u001b[39;00m extract_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning up temporary directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mislink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m     \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mislink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m     \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\shutil.py:622\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    620\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mc:\\Users\\zidti\\anaconda3\\Lib\\shutil.py:620\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'D:\\\\AS3\\\\Compressed_Files\\\\temp_4b38c844ee9c46f5b73ce28bb8f08da7\\\\raw_review_All_Beauty\\\\full\\\\data-00000-of-00001.arrow'"
     ]
    }
   ],
   "source": [
    "# Ensure the function iterates over files in the directory and constructs full paths\n",
    "categories = [\"All_beauty\", \"Appliances\"]\n",
    "for category in categories:\n",
    "    load_raw_rev_and_meta(extraction_path, category)\n",
    "# load_raw_rev_and_meta(extraction_path, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For loop to iterate the categories, clean them and convert to compressed pkl zips\n",
    "# # also removes the uncompressed files from the system once they've been done\n",
    "# for category in categories:\n",
    "#     base_path = r\"\" # replace with path to tar files\n",
    "#     meta_path = r\"\" # replace with path to meta pkl files\n",
    "#     review_path = r\"\" # replace with path to review pkl files\n",
    "\n",
    "#     # review pkled folder\n",
    "#     rev_pkl  = r\"/root/Data/output_folder musical-video_games/reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "#     meta_pkl = r\"/root/Data/output_folder musical-video_games/meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    \n",
    "#     load_compressed_dataset()\n",
    "\n",
    "#     review_df = convert_to_df(review_path, category)\n",
    "#     meta_df = convert_to_df(meta_path, category)\n",
    "#     cleaned = clean_data(category, review_df, meta_df)\n",
    "#     print(cleaned)\n",
    "#     del cleaned\n",
    "#     del meta_df\n",
    "#     del review_df\n",
    "\n",
    "#     # remove the review and meta pkl files that aren't compressed\n",
    "#     if os.path.exists(rev_pkl):\n",
    "#         shutil.rmtree(rev_pkl)\n",
    "#     else:\n",
    "#         print(f\"{rev_pkl} path does not exist\")\n",
    "\n",
    "#     if os.path.exists(meta_pkl):\n",
    "#         shutil.rmtree(meta_pkl)\n",
    "#     else:\n",
    "#         print(f\"{meta_pkl} path does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
