{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 â€“ A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac0f95",
   "metadata": {},
   "source": [
    "## Function for extraction of RAW .tar files, creates .arrow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e03e8b",
   "metadata": {},
   "source": [
    "## Preprocess Category: Creates a temp path for management of Disk Storage <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category, batch_size=1000):\n",
    "    # Define a temp path inside the output folder, unique per category\n",
    "    temp_path = os.path.join(output_folder, \"temp_extract\", category)\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Extracting tar files for {category}...\")\n",
    "\n",
    "    # Extracts tar.bz2\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files in {category}\")\n",
    "\n",
    "    batch_num = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "\n",
    "            pkl_output_path = os.path.join(output_folder, f\"{folder_name}_pkl\")\n",
    "            os.makedirs(pkl_output_path, exist_ok=True)\n",
    "\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "         \n",
    "\n",
    "            for i, row in enumerate(dataset):\n",
    "                if not row:\n",
    "                    continue\n",
    "\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_pickle(os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\"))\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows)\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "                    total_rows += 1\n",
    "\n",
    "            # Final batch\n",
    "            if batch:\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_pickle(os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\"))\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "            print(f\"Finished {arrow_file.name}: saved {total_rows} total rows in {batch_num + 1} batches\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    # Cleanup\n",
    "    if os.path.exists(temp_path):\n",
    "        shutil.rmtree(temp_path)\n",
    "        print(f\"Temp folder removed: {temp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(folder, category):\n",
    "    df_r = []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith(\".pkl\") and category.lower() in fname.lower():\n",
    "            try:\n",
    "                file_path = os.path.join(folder, fname)\n",
    "                review_df = pd.read_pickle(file_path)\n",
    "                print(f\"{fname} loaded: shape = {review_df.shape}\")\n",
    "                df_r.append(review_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {fname}:\", e)\n",
    "\n",
    "    if df_r:\n",
    "        review_df = pd.concat(df_r, ignore_index=True)\n",
    "        print(\"All .pkl files loaded. Final shape:\", review_df.shape)\n",
    "        \n",
    "    print(\"Removed reviews pkl folder\")\n",
    "    return review_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(category, review_df, meta_df):\n",
    "\n",
    "    output_dir = r\"C:\\Users\\maian\\Downloads\\cleaned files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=== Merging review and meta on 'parent_asin' (LEFT JOIN) ===\")\n",
    "    merged = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "    print(f\"After merge: {len(merged)} rows\")\n",
    "\n",
    "    # invalid ratings\n",
    "    if \"rating\" in merged.columns:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "        print(f\"After filtering invalid ratings: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "    \n",
    "    # empty review texts\n",
    "    if \"text\" in merged.columns:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"text\"].notna() & (merged[\"text\"].str.strip() != \"\")]\n",
    "        print(f\"After dropping empty text: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "    #  brand\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(\n",
    "        lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1\n",
    "    )\n",
    "    merged[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    # duplicates\n",
    "    before = len(merged)\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"product_id\", \"text\"], keep=\"first\", inplace=True)\n",
    "    print(f\"After removing duplicates: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "    # derived columns\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    # cleaned data\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "    print(f\"Saved cleaned file to {output_file}\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "## Define Categories that will be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health_and_Household\",\n",
    "    \"Health_and_Personal_Care\",\n",
    "    \"Home_and_Kitchen\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14375f1d",
   "metadata": {},
   "source": [
    "## Running Preprocess then Clean_Data for the Categories defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0959625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\"\n",
    "raw_dir = os.path.join(base_dir, \"raw_files\")\n",
    "output_dir = os.path.join(base_dir, \"output_folder\")\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category} ===\")\n",
    "\n",
    "    # Point to the .tar files in raw_files/\n",
    "    review_tar = os.path.join(raw_dir, f\"raw_review_{category}.tar.bz2\")\n",
    "    meta_tar = os.path.join(raw_dir, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Extract and convert arrow to review/meta .pkl batches\n",
    "        preprocess_category(review_tar, meta_tar, output_dir, category)\n",
    "\n",
    "        # Step 2: Load the .pkl batches for this category\n",
    "        review_df = convert_to_df(os.path.join(output_dir, \"reviews_pkl\"), category)\n",
    "        meta_df = convert_to_df(os.path.join(output_dir, \"meta_pkl\"), category)\n",
    "\n",
    "        # Step 3: Clean and save cleaned output\n",
    "        cleaned = clean_data(category, review_df, meta_df)\n",
    "        print(f\"Cleaned shape: {cleaned.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {category}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Step 4: Free memory\n",
    "        for var in ['cleaned', 'review_df', 'meta_df']:\n",
    "            if var in locals():\n",
    "                del globals()[var]\n",
    "        gc.collect()\n",
    "\n",
    "        #Step 5: Clean up intermediate pkl folders\n",
    "        for sub in [\"reviews_pkl\", \"meta_pkl\", \"temp_extract\"]:\n",
    "            path = os.path.join(output_dir, sub)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    shutil.rmtree(path)\n",
    "                    print(f\"Deleted: {path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Couldn't delete {path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Path does not exist: {path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
