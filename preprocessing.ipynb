{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 â€“ A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b459c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac0f95",
   "metadata": {},
   "source": [
    "## Function for extraction of RAW .tar files, creates .arrow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e03e8b",
   "metadata": {},
   "source": [
    "## Preprocess Category: Creates a temp path for management of Disk Storage <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category, batch_size=1000):\n",
    "    temp_path = os.path.join(output_folder, \"temp_extract\", category)\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Extracting tar files for {category}...\")\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "    batch_num = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "            out_path = os.path.join(output_folder, f\"{folder_name}_parquet\")\n",
    "            os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "\n",
    "            for row in dataset:\n",
    "                if not row:\n",
    "                    continue\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_parquet(os.path.join(out_path, f\"{category}_batch_{batch_num}.parquet\"), index=False)\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows)\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "                    total_rows += 1\n",
    "\n",
    "            if batch:\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_parquet(os.path.join(out_path, f\"{category}_batch_{batch_num}.parquet\"), index=False)\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    shutil.rmtree(temp_path)\n",
    "    print(f\"Temp folder removed: {temp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_dd(folder, category):\n",
    "    files = [os.path.join(folder, f) for f in os.listdir(folder)\n",
    "             if f.endswith(\".parquet\") and category.lower() in f.lower()]\n",
    "    if not files:\n",
    "        print(\"No parquet files found\")\n",
    "        return None\n",
    "    df = dd.read_parquet(files)\n",
    "    print(f\"Loaded {len(files)} files into Dask DataFrame\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_dask(category, review_path, meta_path):\n",
    "    output_dir = r\"C:\\Users\\maian\\Downloads\\cleaned files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Reading parquet files as Dask DataFrames\")\n",
    "    review_df = dd.read_parquet(review_path)\n",
    "    meta_df = dd.read_parquet(meta_path)\n",
    "\n",
    "    print(\"Merging review and meta on 'parent_asin'\")\n",
    "    merged = dd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    if \"rating\" in merged.columns:\n",
    "        merged = merged[merged[\"rating\"].between(1, 5)]\n",
    "    if \"text\" in merged.columns:\n",
    "        merged = merged[merged[\"text\"].notnull() & (merged[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    def safe_extract_brand(row):\n",
    "        try:\n",
    "            return extract_brand(row.get(\"details\"), row.get(\"store\"))\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    print(\"Extracting brand...\")\n",
    "    merged[\"brand\"] = merged.map_partitions(\n",
    "        lambda df: df.apply(lambda row: safe_extract_brand(row), axis=1),\n",
    "        meta=(\"brand\", \"object\")\n",
    "    )\n",
    "    merged[\"brand\"] = merged[\"brand\"].fillna(\"Unknown\")\n",
    "    print(\"Dropping duplicates...\")\n",
    "    merged = merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"])\n",
    "\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().map(lambda x: len(x) if x else 0, meta=(\"review_length\", \"int\"))\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = dd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    necessary_columns = [\n",
    "        \"user_id\", \"asin\", \"parent_asin\", \"rating\", \"text\", \"verified_purchase\",\n",
    "        \"helpful_vote\", \"review_length\", \"year\", \"brand\", \"main_category\",\n",
    "        \"title\", \"average_rating\", \"rating_number\", \"price\"\n",
    "    ]\n",
    "    merged = merged[[col for col in necessary_columns if col in merged.columns]]\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned.parquet\")\n",
    "    merged.to_parquet(output_file, compression=\"snappy\", write_index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "## Define Categories that will be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health_and_Household\",\n",
    "    \"Health_and_Personal_Care\",\n",
    "    \"Home_and_Kitchen\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14375f1d",
   "metadata": {},
   "source": [
    "## Running Preprocess then Clean_Data for the Categories defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0959625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\"\n",
    "raw_dir = os.path.join(base_dir, \"raw_files\")\n",
    "output_dir = os.path.join(base_dir, \"output_folder\")\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category} ===\")\n",
    "\n",
    "    review_tar = os.path.join(raw_dir, f\"raw_review_{category}.tar.bz2\")\n",
    "    meta_tar = os.path.join(raw_dir, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "    try:\n",
    "        preprocess_category(review_tar, meta_tar, output_dir, category)\n",
    "\n",
    "        review_df = convert_to_dd(os.path.join(output_dir, \"reviews_parquet\"), category)\n",
    "        meta_df = convert_to_dd(os.path.join(output_dir, \"meta_parquet\"), category)\n",
    "\n",
    "        if review_df is not None and meta_df is not None:\n",
    "            cleaned = clean_data_dask(category, review_df, meta_df)\n",
    "            print(f\"Cleaned {category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {category}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        for sub in [\"reviews_parquet\", \"meta_parquet\", \"temp_extract\"]:\n",
    "            path = os.path.join(output_dir, sub)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    shutil.rmtree(path)\n",
    "                    print(f\"Deleted: {path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Couldn't delete {path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
