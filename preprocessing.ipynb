{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 â€“ A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dda1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install pyspark\n",
    "# %pip install findspark\n",
    "# %pip install -q gdown\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install pyarrow\n",
    "# %pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b23bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "# # import `DenseVector`\n",
    "# from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# # import `StandardScaler`\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# sudo apt install python3-distutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b459c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80463dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder\\\n",
    "# .appName(\"Amazon_Reviews\")\\\n",
    "# .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932c7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_schema = StructType([\n",
    "#     StructField(\"rating\", FloatType(), True),\n",
    "#     StructField(\"title\", StringType(), True),\n",
    "#     StructField(\"images\", ArrayType(StringType()), True),\n",
    "#     StructField(\"asin\", StringType(), True),\n",
    "#     StructField(\"parent_asin\", FloatType(), True),\n",
    "#     StructField(\"user_id\", ArrayType(StringType()), True),\n",
    "#     StructField(\"timestamp\", IntegerType(), True),\n",
    "#     StructField(\"verified_purchase\", BooleanType(), True),\n",
    "#     StructField(\"helpful_vote\", StringType(), True),\n",
    "# ])\n",
    "\n",
    "# # String types in arrays may need to be sequence but couldn't find the actual sequence dytpe syntax\n",
    "# meta_schema = StructType([\n",
    "#     StructField(\"main_category\", StringType(), True),\n",
    "#     StructField(\"title\", StringType(), True),\n",
    "#     StructField(\"average_rating\", FloatType(), True),\n",
    "#     StructField(\"rating_number\", IntegerType(), True),\n",
    "#     StructField(\"features\", ArrayType(StringType()), True),\n",
    "#     StructField(\"description\", ArrayType(StringType()), True),\n",
    "#     StructField(\"price\", FloatType(), True),\n",
    "#     StructField(\"images\", ArrayType(StringType()), True),\n",
    "#     StructField(\"videos\", ArrayType(StringType()), True),\n",
    "#     StructField(\"store\", StringType(), True),\n",
    "#     StructField(\"categories\", ArrayType(StringType()), True),\n",
    "#     StructField(\"details\", MapType(StringType(), IntegerType()), True),\n",
    "#     StructField(\"parent_asin\", FloatType(), True),\n",
    "#     StructField(\"user_id\", ArrayType(StringType()), True),\n",
    "#     StructField(\"bought_together\", ArrayType(StringType()), True),\n",
    "#     # StructField(\"timestamp\", IntegerType(), True),\n",
    "#     # StructField(\"verified_purchase\", BooleanType(), True),\n",
    "#     # StructField(\"helpful_vote\", StringType(), True),\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb0eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar_folder = 'root/Data'\n",
    "output_folder = 'root/output_folder'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category, batch_size=1000):\n",
    "    # Define a temp path inside the output folder, unique per category\n",
    "    temp_path = os.path.join(output_folder, \"temp_extract\", category)\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Extracting tar files for {category}...\")\n",
    "\n",
    "    # Make sure you have a function to extract tar.bz2\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files in {category}\")\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "\n",
    "            pkl_output_path = os.path.join(output_folder, f\"{folder_name}_pkl\")\n",
    "            os.makedirs(pkl_output_path, exist_ok=True)\n",
    "\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "            batch_num = 0\n",
    "\n",
    "            for i, row in enumerate(dataset):\n",
    "                if not row:\n",
    "                    continue\n",
    "\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_pickle(os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\"))\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows)\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "\n",
    "            # Final batch\n",
    "            if batch:\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_pickle(os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\"))\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    # Cleanup\n",
    "    if os.path.exists(temp_path):\n",
    "        shutil.rmtree(temp_path)\n",
    "        print(f\"Temp folder removed: {temp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872f022",
   "metadata": {},
   "source": [
    "Calling fn to preprocess for a category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a80c9",
   "metadata": {},
   "source": [
    "Run for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88009925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_category(\n",
    "#     r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_meta_All_Beauty.tar.bz2\",\n",
    "#     r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_review_All_Beauty.tar.bz2\",\n",
    "#     \"output_folder\", category=\"All_Beauty\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4acd4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_meta_df(folder):\n",
    "#     df_m = []\n",
    "\n",
    "#     for fname in sorted(os.listdir(folder)):\n",
    "#         if fname.endswith(\".pkl\"):\n",
    "#             try:\n",
    "#                 file_path = os.path.join(folder, fname)\n",
    "#                 meta_df = pd.read_pickle(file_path)\n",
    "#                 print(f\"{fname} loaded: shape = {meta_df.shape}\")\n",
    "#                 df_m.append(meta_df)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in {fname}:\", e)\n",
    "\n",
    "#     if df_m:\n",
    "#         meta_df = pd.concat(df_m, ignore_index=True)\n",
    "#         print(\"All .pkl files loaded. Final shape:\", meta_df.shape)\n",
    "        \n",
    "#     print(\"Removed meta pkl folder\")\n",
    "#     return meta_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(folder, category):\n",
    "    df_r = []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith(\".pkl\") and category.lower() in fname.lower():\n",
    "            try:\n",
    "                file_path = os.path.join(folder, fname)\n",
    "                review_df = pd.read_pickle(file_path)\n",
    "                print(f\"{fname} loaded: shape = {review_df.shape}\")\n",
    "                df_r.append(review_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {fname}:\", e)\n",
    "\n",
    "    if df_r:\n",
    "        review_df = pd.concat(df_r, ignore_index=True)\n",
    "        print(\"All .pkl files loaded. Final shape:\", review_df.shape)\n",
    "        \n",
    "    print(\"Removed reviews pkl folder\")\n",
    "    return review_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621964",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462347ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(\n",
    "#     review_df,\n",
    "#     meta_df,\n",
    "#     on=\"parent_asin\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(category, review_df, meta_df):\n",
    "\n",
    "    output_dir = r\"C:\\Users\\maian\\Downloads\\cleaned files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=== Merging review and meta on 'parent_asin' (LEFT JOIN) ===\")\n",
    "    merged = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "    print(f\"After merge: {len(merged)} rows\")\n",
    "\n",
    "    # Drop invalid ratings\n",
    "    if \"rating\" in merged.columns:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "        print(f\"After filtering invalid ratings: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "    \n",
    "    # Drop empty review texts\n",
    "    if \"text\" in merged.columns:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"text\"].notna() & (merged[\"text\"].str.strip() != \"\")]\n",
    "        print(f\"After dropping empty text: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "    # Extract brand\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(\n",
    "        lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1\n",
    "    )\n",
    "    merged[\"brand\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    before = len(merged)\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "    print(f\"After removing duplicates: {len(merged)} rows (dropped {before - len(merged)})\")\n",
    "\n",
    "    # Add derived columns\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    # Save cleaned data\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "    print(f\"Saved cleaned file to {output_file}\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52287f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned = clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "Run for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health_and_Household\",\n",
    "    \"Health_and_Personal_Care\",\n",
    "    \"Home_and_Kitchen\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0959625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing category: Grocery_and_Gourmet_Food ===\n",
      "Extracting tar files for Grocery_and_Gourmet_Food...\n",
      "Extracting C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\raw_files\\raw_review_Grocery_and_Gourmet_Food.tar.bz2 to C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\output_folder\\temp_extract\\Grocery_and_Gourmet_Food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maian\\AppData\\Local\\Temp\\ipykernel_16252\\2872024491.py:12: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path does not exist: C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\output_folder\\reviews_pkl\n",
      "Path does not exist: C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\output_folder\\meta_pkl\n",
      "Couldn't delete C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\\output_folder\\temp_extract: [WinError 5] Access is denied: 'C:\\\\Users\\\\maian\\\\OneDrive - The University of the West Indies, St. Augustine\\\\Desktop\\\\big_data_a3\\\\output_folder\\\\temp_extract'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m meta_tar \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_meta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tar.bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Step 1: Extract and convert arrow to review/meta .pkl batches\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mpreprocess_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_tar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_tar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Step 2: Load the .pkl batches for this category\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     review_df \u001b[38;5;241m=\u001b[39m convert_to_df(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews_pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m), category)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mpreprocess_category\u001b[1;34m(review_tar_path, meta_tar_path, output_folder, category, batch_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting tar files for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Make sure you have a function to extract tar.bz2\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mextract_tar_bz2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_tar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m extract_tar_bz2(meta_tar_path, temp_path)\n\u001b[0;32m     20\u001b[0m arrow_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(Path(temp_path)\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36mextract_tar_bz2\u001b[1;34m(tar_path, extract_dir)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(tar_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr:bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtar_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m         \u001b[43mtar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during extraction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tarfile.py:2334\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[1;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[0;32m   2330\u001b[0m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[0;32m   2331\u001b[0m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[0;32m   2332\u001b[0m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[0;32m   2333\u001b[0m         directories\u001b[38;5;241m.\u001b[39mappend(tarinfo)\n\u001b[1;32m-> 2334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2335\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tarfile.py:2397\u001b[0m, in \u001b[0;36mTarFile._extract_one\u001b[1;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2397\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2398\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_fatal_error(e)\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tarfile.py:2480\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[1;32m-> 2480\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[0;32m   2482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tarfile.py:2534\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[1;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[0;32m   2532\u001b[0m     target\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2534\u001b[0m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mReadError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tarfile.py:253\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m bufsize:\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m     \u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remainder \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    256\u001b[0m     buf \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(remainder)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\"\n",
    "raw_dir = os.path.join(base_dir, \"raw_files\")\n",
    "output_dir = os.path.join(base_dir, \"output_folder\")\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category} ===\")\n",
    "\n",
    "    # Point to the .tar files in raw_files/\n",
    "    review_tar = os.path.join(raw_dir, f\"raw_review_{category}.tar.bz2\")\n",
    "    meta_tar = os.path.join(raw_dir, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Extract and convert arrow to review/meta .pkl batches\n",
    "        preprocess_category(review_tar, meta_tar, output_dir, category)\n",
    "\n",
    "        # Step 2: Load the .pkl batches for this category\n",
    "        review_df = convert_to_df(os.path.join(output_dir, \"reviews_pkl\"), category)\n",
    "        meta_df = convert_to_df(os.path.join(output_dir, \"meta_pkl\"), category)\n",
    "\n",
    "        # Step 3: Clean and save cleaned output\n",
    "        cleaned = clean_data(category, review_df, meta_df)\n",
    "        print(f\"Cleaned shape: {cleaned.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {category}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Step 4: Free memory\n",
    "        for var in ['cleaned', 'review_df', 'meta_df']:\n",
    "            if var in locals():\n",
    "                del globals()[var]\n",
    "        gc.collect()\n",
    "\n",
    "        # Step 5: Clean up intermediate pkl folders\n",
    "        for sub in [\"reviews_pkl\", \"meta_pkl\", \"temp_extract\"]:\n",
    "            path = os.path.join(output_dir, sub)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    shutil.rmtree(path)\n",
    "                    print(f\"Deleted: {path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Couldn't delete {path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Path does not exist: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4675a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\maian\\\\Downloads\\\\Musical_Instruments_cleaned_merged.pkl.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmaian\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMusical_Instruments_cleaned_merged.pkl.bz2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbz2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:783\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;66;03m# BZ Compression\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbz2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# Overload of \"BZ2File\" to handle pickle protocol 5\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43mget_bz2_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# ZIP Compression\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_BytesZipFile\" has incompatible type\u001b[39;00m\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[39;00m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;66;03m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maian\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\bz2.py:81\u001b[0m, in \u001b[0;36mBZ2File.__init__\u001b[1;34m(self, filename, mode, compresslevel)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (mode,))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp \u001b[38;5;241m=\u001b[39m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closefp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode_code\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\maian\\\\Downloads\\\\Musical_Instruments_cleaned_merged.pkl.bz2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(r\"C:\\Users\\maian\\Downloads\\Musical_Instruments_cleaned_merged.pkl.bz2\", compression=\"bz2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
