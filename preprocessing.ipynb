{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 â€“ A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dda1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark\n",
    "# %pip install findspark\n",
    "# %pip install -q gdown\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install pyarrow\n",
    "# %pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "# # import `DenseVector`\n",
    "# from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# # import `StandardScaler`\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# # sudo apt install python3-distutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bigdata_a3_utils.py'; 'bigdata_a3_utils' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbigdata_a3_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bigdata_a3_utils.py'; 'bigdata_a3_utils' is not a package"
     ]
    }
   ],
   "source": [
    "from bigdata_a3_utils.py import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459c240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb0eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_folder = 'root/Data'\n",
    "output_folder = 'root/output_folder'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category, batch_size=1000):\n",
    "    temp_path = \"Data/temp_extract\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(\"Extracting tar files...\")\n",
    "        extract_tar_bz2(review_tar_path, temp_path)\n",
    "        extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "        arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "        print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "        if not arrow_files:\n",
    "            print(\"No Arrow files found, skipping processing.\")\n",
    "            return\n",
    "\n",
    "        for arrow_file in arrow_files:\n",
    "            print(f\"Processing: {arrow_file.name}\")\n",
    "            try:\n",
    "                is_meta = \"meta\" in str(arrow_file).lower()\n",
    "                folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "\n",
    "                pkl_output_path = os.path.join(output_folder, f\"{folder_name}_pkl\")\n",
    "                os.makedirs(pkl_output_path, exist_ok=True)\n",
    "\n",
    "                dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "                batch = []\n",
    "                seen_keys = set()\n",
    "                batch_num = 0\n",
    "                row_count = 0\n",
    "\n",
    "                for i, row in enumerate(dataset):\n",
    "                    if not row:\n",
    "                        continue\n",
    "\n",
    "                    if not is_meta:\n",
    "                        key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                        if None in key:\n",
    "                            print(f\"Missing key fields in row {i}: {key}\")\n",
    "                            continue\n",
    "                        if key in seen_keys:\n",
    "                            continue\n",
    "                        seen_keys.add(key)\n",
    "\n",
    "                    batch.append(row)\n",
    "                    row_count += 1\n",
    "\n",
    "                    if len(batch) >= batch_size:\n",
    "                        df = pd.DataFrame(batch)\n",
    "                        output_file = os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\")\n",
    "                        df.to_pickle(output_file)\n",
    "                        print(f\"Saved batch {batch_num} with {len(batch)} rows to {output_file}\")\n",
    "                        batch = []\n",
    "                        batch_num += 1\n",
    "\n",
    "                if batch:\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    output_file = os.path.join(pkl_output_path, f\"{category}_batch_{batch_num}.pkl\")\n",
    "                    df.to_pickle(output_file)\n",
    "                    print(f\"Saved final batch {batch_num} with {len(batch)} rows to {output_file}\")\n",
    "\n",
    "                print(f\"Finished processing {arrow_file.name} with {row_count} total rows (after dedup)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Top-level error during preprocessing: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            shutil.rmtree(temp_path)\n",
    "            print(\"ðŸ§¹ Temp folder removed.\")\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872f022",
   "metadata": {},
   "source": [
    "Calling fn to preprocess for a category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a80c9",
   "metadata": {},
   "source": [
    "Run for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88009925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_category(\n",
    "#     r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_meta_All_Beauty.tar.bz2\",\n",
    "#     r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_review_All_Beauty.tar.bz2\",\n",
    "#     \"output_folder\", category=\"All_Beauty\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4acd4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_meta_df(folder):\n",
    "    df_m = []\n",
    "\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith(\".pkl\"):\n",
    "            try:\n",
    "                file_path = os.path.join(folder, fname)\n",
    "                meta_df = pd.read_pickle(file_path)\n",
    "                print(f\"{fname} loaded: shape = {meta_df.shape}\")\n",
    "                df_m.append(meta_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {fname}:\", e)\n",
    "\n",
    "    if df_m:\n",
    "        meta_df = pd.concat(df_m, ignore_index=True)\n",
    "        print(\"All .pkl files loaded. Final shape:\", meta_df.shape)\n",
    "        \n",
    "    print(\"Removed meta pkl folder\")\n",
    "    return meta_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_rev_df(folder):\n",
    "    df_r = []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith(\".pkl\"):\n",
    "            try:\n",
    "                file_path = os.path.join(folder, fname)\n",
    "                review_df = pd.read_pickle(file_path)\n",
    "                print(f\"{fname} loaded: shape = {review_df.shape}\")\n",
    "                df_r.append(review_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {fname}:\", e)\n",
    "\n",
    "    if df_r:\n",
    "        review_df = pd.concat(df_r, ignore_index=True)\n",
    "        print(\"All .pkl files loaded. Final shape:\", review_df.shape)\n",
    "        \n",
    "    print(\"Removed reviews pkl folder\")\n",
    "    return review_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621964",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462347ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(\n",
    "#     review_df,\n",
    "#     meta_df,\n",
    "#     on=\"parent_asin\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(category, review_df, meta_df):\n",
    "    output_dir = r\"D:/UWI/Year 3/Sem 2/COMP3610-Big-Data/Assignments/Assignment#3/A3/datasets/output_folder/cleaned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"inner\")\n",
    "    print(\"Merged\")\n",
    "\n",
    "    print(\"Filtering invalid ratings...\")\n",
    "    merged_df = merged_df[merged_df[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "    print(\"Dropping empty review text...\")\n",
    "    merged = merged_df[merged_df[\"text\"].notna() & (merged_df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "    print(\"Removing duplicate reviews...\")\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    print(\"Computing review length...\")\n",
    "    merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    print(\"Extracting year from timestamp...\")\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "    print(\" All cleaning steps completed.\")\n",
    "    \n",
    "    test = merged\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52287f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned = clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "Run for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [\"Clothing_Shoes_and_Jewelry\"]\n",
    "categories = [\"All_Beauty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0959625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar files...\n",
      "Error: File D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_meta_All_Beauty.tar.bz2 does not exist.\n",
      "Error: File D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_review_All_Beauty.tar.bz2 does not exist.\n",
      "Found 0 Arrow files\n",
      "No Arrow files found, skipping processing.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\UWI\\\\Year 3\\\\Sem 2\\\\COMP3610-Big-Data\\\\Assignments\\\\Assignment#3\\\\A3\\\\Code\\\\output_folder\\\\reviews_pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m meta_pkl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUWI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mYear 3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSem 2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCOMP3610-Big-Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAssignments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAssignment#3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mA3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput_folder\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmeta_pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Make sure this is the folder with meta .pkl batches\u001b[39;00m\n\u001b[0;32m     12\u001b[0m preprocess_category(meta_path, review_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m, category)\n\u001b[1;32m---> 13\u001b[0m review_df \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_rev_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrev_pkl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m meta_df \u001b[38;5;241m=\u001b[39m convert_to_meta_df(meta_pkl)\n\u001b[0;32m     15\u001b[0m cleaned \u001b[38;5;241m=\u001b[39m clean_data(category, review_df, meta_df)\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mconvert_to_rev_df\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_to_rev_df\u001b[39m(folder):\n\u001b[0;32m      2\u001b[0m     df_r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\UWI\\\\Year 3\\\\Sem 2\\\\COMP3610-Big-Data\\\\Assignments\\\\Assignment#3\\\\A3\\\\Code\\\\output_folder\\\\reviews_pkl'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for category in categories:\n",
    "    base_path = r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\"\n",
    "    meta_path = os.path.join(base_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "    review_path = os.path.join(base_path, f\"raw_review_{category}.tar.bz2\")\n",
    "\n",
    "    # review pkled folder\n",
    "    rev_pkl  = r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\Code\\output_folder\\reviews_pkl\" # Make sure this is the folder with review .pkl batches\n",
    "    meta_pkl = r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\Code\\output_folder\\meta_pkl\"  # Make sure this is the folder with meta .pkl batches\n",
    "\n",
    "    preprocess_category(meta_path, review_path, \"output_folder\", category)\n",
    "    review_df = convert_to_rev_df(rev_pkl)\n",
    "    meta_df = convert_to_meta_df(meta_pkl)\n",
    "    cleaned = clean_data(category, review_df, meta_df)\n",
    "    print(cleaned)\n",
    "    del cleaned\n",
    "    del meta_df\n",
    "    del review_df\n",
    "\n",
    "    # remove the review and meta pkl files that aren't compressed\n",
    "    if os.path.exists(rev_pkl):\n",
    "        shutil.rmtree(rev_pkl)\n",
    "    else:\n",
    "        print(f\"{rev_pkl} path does not exist\")\n",
    "    \n",
    "    if os.path.exists(meta_pkl):\n",
    "        shutil.rmtree(meta_pkl)\n",
    "    else:\n",
    "        print(f\"{meta_pkl} path does not exist\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
