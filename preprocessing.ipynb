{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 â€“ A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b459c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e2cdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:45621' processes=4 threads=4, memory=15.62 GiB>\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac0f95",
   "metadata": {},
   "source": [
    "## Function for extraction of RAW .tar files, creates .arrow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e03e8b",
   "metadata": {},
   "source": [
    "## Preprocess Category: Creates a temp path for management of Disk Storage <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category, batch_size=1000):\n",
    "    temp_path = os.path.join(output_folder, \"temp_extract\", category)\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Extracting tar files for {category}...\")\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "    batch_num = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "            out_path = os.path.join(output_folder, f\"{folder_name}_parquet\")\n",
    "            os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "\n",
    "            for row in dataset:\n",
    "                if not row:\n",
    "                    continue\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_parquet(os.path.join(out_path, f\"{category}_batch_{batch_num}.parquet\"), index=False)\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows)\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "                    total_rows += 1\n",
    "\n",
    "            if batch:\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_parquet(os.path.join(out_path, f\"{category}_batch_{batch_num}.parquet\"), index=False)\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    shutil.rmtree(temp_path)\n",
    "    print(f\"Temp folder removed: {temp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_dd(folder, category):\n",
    "    files = [os.path.join(folder, f) for f in os.listdir(folder)\n",
    "             if f.endswith(\".parquet\") and category.lower() in f.lower()]\n",
    "    if not files:\n",
    "        print(\"No parquet files found\")\n",
    "        return None\n",
    "    df = dd.read_parquet(files)\n",
    "    print(f\"Loaded {len(files)} files into Dask DataFrame\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import subprocess\n",
    "from dask.diagnostics import ProgressBar\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def clean_data_dask(category, review_df, meta_df):\n",
    "    print(\"Merging review and meta on 'parent_asin'\")\n",
    "    merged = dd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    print(\"Filtering bad data\")\n",
    "    if \"rating\" in merged.columns:\n",
    "        merged = merged[merged[\"rating\"].between(1, 5)]\n",
    "    if \"text\" in merged.columns:\n",
    "        merged = merged[merged[\"text\"].notnull() & (merged[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand\")\n",
    "    def fast_extract_brand(details, store):\n",
    "        if isinstance(details, dict) and details.get(\"brand\"):\n",
    "            return details[\"brand\"]\n",
    "        elif isinstance(store, str) and store.strip():\n",
    "            return store\n",
    "        return \"Unknown\"\n",
    "\n",
    "    merged[\"brand\"] = merged.map_partitions(\n",
    "        lambda df: df.apply(lambda row: fast_extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1),\n",
    "        meta=(\"brand\", \"object\")\n",
    "    )\n",
    "\n",
    "    print(\"Computing derived columns\")\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().map(\n",
    "            lambda x: len(x) if x else 0, meta=(\"review_length\", \"int\")\n",
    "        )\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = dd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    print(\"Selecting necessary columns\")\n",
    "    necessary_columns = [\n",
    "        \"user_id\", \"asin\", \"parent_asin\", \"rating\", \"text\", \"verified_purchase\",\n",
    "        \"helpful_vote\", \"review_length\", \"year\", \"brand\", \"main_category\",\n",
    "        \"title\", \"average_rating\", \"rating_number\", \"price\"\n",
    "    ]\n",
    "    merged = merged[[col for col in necessary_columns if col in merged.columns]]\n",
    "\n",
    "    print(\"Casting columns to match the expected schema\")\n",
    "    dtype_mapping = {\n",
    "        \"verified_purchase\": \"string\",\n",
    "        \"helpful_vote\": \"float64\",\n",
    "        \"rating_number\": \"float64\",\n",
    "        \"price\": \"string\",\n",
    "    }\n",
    "    merged = merged.astype(dtype_mapping)\n",
    "\n",
    "    print(\"Repartitioning to a single partition for saving as one file\")\n",
    "    merged = merged.repartition(npartitions=1)\n",
    "\n",
    "    print(\"Writing Parquet data to memory\")\n",
    "    buffer = BytesIO()\n",
    "    with ProgressBar():\n",
    "        merged.compute().to_parquet(buffer, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "\n",
    "    print(\"Uploading Parquet data to Google Drive\")\n",
    "    output_file = f\"{category}_cleaned.parquet\"\n",
    "    try:\n",
    "        # Write the buffer to a temporary file for rclone\n",
    "        temp_file = f\"/tmp/{output_file}\"\n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            f.write(buffer.getvalue())\n",
    "\n",
    "        # Use rclone to upload the file\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"rclone\", \"copy\", temp_file,\n",
    "                \"googdrive:\", \"--drive-shared-with-me\", \"--drive-root-folder-id\", \"1wgVfpqS9BJE2IvTN-BJSbT3kEbhMhaOC\"\n",
    "            ],\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"Uploaded {output_file} to Google Drive folder: cleaned\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error uploading to Google Drive: {e}\")\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "## Define Categories that will be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [\n",
    "#     \"Grocery_and_Gourmet_Food\",\n",
    "#     \"Handmade_Products\",\n",
    "#     \"Health_and_Household\",\n",
    "#     \"Home_and_Kitchen\",\n",
    "#     \"Industrial_and_Scientific\",\n",
    "#     \"Kindle_Store\",\n",
    "#     \"Magazine_Subscriptions\",\n",
    "#     \"Movies_and_TV\",\n",
    "#     \"Musical_Instruments\"\n",
    "# ]\n",
    "\n",
    "categories = [\n",
    "    \"Subscription_Boxes\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14375f1d",
   "metadata": {},
   "source": [
    "## Running Preprocess then Clean_Data for the Categories defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0959625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing category: Subscription_Boxes ===\n",
      "Extracting tar files for Subscription_Boxes...\n",
      "Extracting /root/Data/raw_review_Subscription_Boxes.tar.bz2 to /root/output_folder/temp_extract/Subscription_Boxes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/Data/raw_meta_Subscription_Boxes.tar.bz2 to /root/output_folder/temp_extract/Subscription_Boxes\n",
      "Found 2 Arrow files\n",
      "Saved batch 0 (1000 rows)\n",
      "Saved batch 1 (1000 rows)\n",
      "Saved batch 2 (1000 rows)\n",
      "Saved batch 3 (1000 rows)\n",
      "Saved batch 4 (1000 rows)\n",
      "Saved batch 5 (1000 rows)\n",
      "Saved batch 6 (1000 rows)\n",
      "Saved batch 7 (1000 rows)\n",
      "Saved batch 8 (1000 rows)\n",
      "Saved batch 9 (1000 rows)\n",
      "Saved batch 10 (1000 rows)\n",
      "Saved batch 11 (1000 rows)\n",
      "Saved batch 12 (1000 rows)\n",
      "Saved batch 13 (1000 rows)\n",
      "Saved batch 14 (1000 rows)\n",
      "Saved batch 15 (1000 rows)\n",
      "Saved final batch 16 (16 rows)\n",
      "Saved final batch 16 (641 rows)\n",
      "Temp folder removed: /root/output_folder/temp_extract/Subscription_Boxes\n",
      "Loaded 17 files into Dask DataFrame\n",
      "Loaded 1 files into Dask DataFrame\n",
      "Merging review and meta on 'parent_asin'\n",
      "Filtering bad data\n",
      "Extracting brand\n",
      "Computing derived columns\n",
      "Selecting necessary columns\n",
      "Casting columns to match the expected schema\n",
      "Repartitioning to a single partition for saving as one file\n",
      "Writing Parquet data to memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/root/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/root/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/root/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Parquet data to Google Drive\n",
      "Uploaded Subscription_Boxes_cleaned.parquet to Google Drive folder: cleaned\n",
      "Cleaned and stored: Subscription_Boxes\n",
      "Deleted: /root/output_folder/reviews_parquet\n",
      "Deleted: /root/output_folder/meta_parquet\n",
      "Deleted: /root/output_folder/temp_extract\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = r\"/root\"\n",
    "raw_dir = os.path.join(base_dir, \"Data\")\n",
    "output_dir = os.path.join(base_dir, \"output_folder\")\n",
    "\n",
    "# Store cleaned Dask DataFrames for each category\n",
    "cleaned_all_categories = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category} ===\")\n",
    "\n",
    "    review_tar = os.path.join(raw_dir, f\"raw_review_{category}.tar.bz2\")\n",
    "    meta_tar = os.path.join(raw_dir, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "    try:\n",
    "        preprocess_category(review_tar, meta_tar, output_dir, category)\n",
    "\n",
    "        review_df = convert_to_dd(os.path.join(output_dir, \"reviews_parquet\"), category)\n",
    "        meta_df = convert_to_dd(os.path.join(output_dir, \"meta_parquet\"), category)\n",
    "\n",
    "        if review_df is not None and meta_df is not None:\n",
    "            cleaned = clean_data_dask(category, review_df, meta_df)\n",
    "            cleaned_all_categories[category] = cleaned\n",
    "            print(f\"Cleaned and stored: {category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {category}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        for sub in [\"reviews_parquet\", \"meta_parquet\", \"temp_extract\"]:\n",
    "            path = os.path.join(output_dir, sub)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    shutil.rmtree(path)\n",
    "                    print(f\"Deleted: {path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Couldn't delete {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edc5db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dfs must be a list of DataFrames/Series objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m combined_cleaned_df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_all_categories\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/dask/dataframe/dask_expr/_collection.py:5496\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(dfs, axis, join, ignore_unknown_divisions, ignore_order, interleave_partitions, **kwargs)\u001b[0m\n\u001b[1;32m   5403\u001b[0m \u001b[38;5;124;03m\"\"\"Concatenate DataFrames along rows.\u001b[39;00m\n\u001b[1;32m   5404\u001b[0m \n\u001b[1;32m   5405\u001b[0m \u001b[38;5;124;03m- When axis=0 (default), concatenate DataFrames row-wise:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m \u001b[38;5;124;03mCategoricalDtype(categories=['a', 'b', 'c'], ordered=False, categories_dtype=object)\u001b[39;00m\n\u001b[1;32m   5494\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dfs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 5496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfs must be a list of DataFrames/Series objects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: dfs must be a list of DataFrames/Series objects"
     ]
    }
   ],
   "source": [
    "combined_cleaned_df = dd.concat(cleaned_all_categories.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8152e7",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Dask-compatible EDA...\")\n",
    "\n",
    "  # Ensure ddf is defined before using it\n",
    "if combined_cleaned_df is not None:\n",
    "    # Star Rating Histogram (1â€“5)\n",
    "    rating_counts =  combined_cleaned_df[\"rating\"].value_counts().compute().sort_index()\n",
    "    rating_counts.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Star Rating\")\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    plt.title(\"Distribution of Star Ratings\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: 'ddf' is not defined. Please define 'ddf' before running this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c094f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_cleaned_df is not None:\n",
    "    top_categories = combined_cleaned_df[\"main_category\"].value_counts().compute().head(10)\n",
    "    top_categories.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Main Category\")\n",
    "    plt.ylabel(\"Review Count\")\n",
    "    plt.title(\"Top 10 Categories by Review Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62879e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"brand\" in cleaned.columns:\n",
    "    top_brands = combined_cleaned_df[combined_cleaned_df[\"brand\"] != \"Unknown\"][\"brand\"].value_counts().compute().head(10)\n",
    "    top_brands.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Brand\")\n",
    "    plt.ylabel(\"Review Count\")\n",
    "    plt.title(\"Top 10 Brands by Review Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"year\" in combined_cleaned_df.columns:\n",
    "    yearly_avg = combined_cleaned_df.groupby(\"year\")[\"rating\"].mean().compute()\n",
    "    yearly_avg.plot(kind=\"line\", marker='o')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Average Rating\")\n",
    "    plt.title(\"Average Rating Over Time\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"review_length\" in combined_cleaned_df.columns:\n",
    "    corr = combined_cleaned_df[[\"review_length\", \"rating\"]].corr().compute()\n",
    "    print(f\"Pearson correlation between review length and rating: {corr.loc['review_length', 'rating']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
