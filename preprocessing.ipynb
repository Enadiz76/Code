{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 â€“ A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac0f95",
   "metadata": {},
   "source": [
    "## Function for extraction of RAW .tar files, creates .arrow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e03e8b",
   "metadata": {},
   "source": [
    "## Preprocess Category: Creates a temp path for management of Disk Storage <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category, batch_size=1000):\n",
    "    temp_path = os.path.join(output_folder, \"temp_extract\", category)\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Extracting tar files for {category}...\")\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "    batch_num = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "            out_path = os.path.join(output_folder, f\"{folder_name}_parquet\")\n",
    "            os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "\n",
    "            for row in dataset:\n",
    "                if not row:\n",
    "                    continue\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_parquet(os.path.join(out_path, f\"{category}_batch_{batch_num}.parquet\"), index=False)\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows)\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "                    total_rows += 1\n",
    "\n",
    "            if batch:\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_parquet(os.path.join(out_path, f\"{category}_batch_{batch_num}.parquet\"), index=False)\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    shutil.rmtree(temp_path)\n",
    "    print(f\"Temp folder removed: {temp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_dd(folder, category):\n",
    "    files = [os.path.join(folder, f) for f in os.listdir(folder)\n",
    "             if f.endswith(\".parquet\") and category.lower() in f.lower()]\n",
    "    if not files:\n",
    "        print(\"No parquet files found\")\n",
    "        return None\n",
    "    df = dd.read_parquet(files)\n",
    "    print(f\"Loaded {len(files)} files into Dask DataFrame\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def clean_data_dask(category, review_path, meta_path):\n",
    "    output_dir = r\"C:\\Users\\maian\\Downloads\\cleaned_files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Reading parquet files as Dask DataFrames\")\n",
    "    review_df = dd.read_parquet(review_path)\n",
    "    meta_df = dd.read_parquet(meta_path)\n",
    "\n",
    "    print(\"Merging review and meta on 'parent_asin'\")\n",
    "    merged = dd.merge(review_df, meta_df, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    print(\"Filtering bad data\")\n",
    "    if \"rating\" in merged.columns:\n",
    "        merged = merged[merged[\"rating\"].between(1, 5)]\n",
    "    if \"text\" in merged.columns:\n",
    "        merged = merged[merged[\"text\"].notnull() & (merged[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand\")\n",
    "    def fast_extract_brand(details, store):\n",
    "        if isinstance(details, dict) and details.get(\"brand\"):\n",
    "            return details[\"brand\"]\n",
    "        elif isinstance(store, str) and store.strip():\n",
    "            return store\n",
    "        return \"Unknown\"\n",
    "\n",
    "    merged[\"brand\"] = merged.map_partitions(\n",
    "        lambda df: df.apply(lambda row: fast_extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1),\n",
    "        meta=(\"brand\", \"object\")\n",
    "    )\n",
    "\n",
    "    print(\"Computing derived columns\")\n",
    "    if \"text\" in merged.columns:\n",
    "        merged[\"review_length\"] = merged[\"text\"].str.split().map(\n",
    "            lambda x: len(x) if x else 0, meta=(\"review_length\", \"int\")\n",
    "        )\n",
    "    if \"timestamp\" in merged.columns:\n",
    "        merged[\"year\"] = dd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    print(\"Selecting necessary columns\")\n",
    "    necessary_columns = [\n",
    "        \"user_id\", \"asin\", \"parent_asin\", \"rating\", \"text\", \"verified_purchase\",\n",
    "        \"helpful_vote\", \"review_length\", \"year\", \"brand\", \"main_category\",\n",
    "        \"title\", \"average_rating\", \"rating_number\", \"price\"\n",
    "    ]\n",
    "    merged = merged[[col for col in necessary_columns if col in merged.columns]]\n",
    "\n",
    "    # print(\"Repartitioning to reduce write overhead\")\n",
    "    # merged = merged.repartition(npartitions=50)\n",
    "\n",
    "    # output_file = os.path.join(output_dir, f\"{category}_cleaned.parquet\")\n",
    "    # print(f\"Saving cleaned file to {output_file}\")\n",
    "\n",
    "    # with ProgressBar():\n",
    "    #     merged.to_parquet(output_file, compression=\"snappy\", write_index=False, overwrite=True)\n",
    "\n",
    "    # print(f\"Done! File saved to: {output_file}\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "## Define Categories that will be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health_and_Household\",\n",
    "    \"Home_and_Kitchen\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14375f1d",
   "metadata": {},
   "source": [
    "## Running Preprocess then Clean_Data for the Categories defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0959625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = r\"C:\\Users\\maian\\OneDrive - The University of the West Indies, St. Augustine\\Desktop\\big_data_a3\"\n",
    "raw_dir = os.path.join(base_dir, \"raw_files\")\n",
    "output_dir = os.path.join(base_dir, \"output_folder\")\n",
    "\n",
    "# Store cleaned Dask DataFrames for each category\n",
    "cleaned_all_categories = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category} ===\")\n",
    "\n",
    "    review_tar = os.path.join(raw_dir, f\"raw_review_{category}.tar.bz2\")\n",
    "    meta_tar = os.path.join(raw_dir, f\"raw_meta_{category}.tar.bz2\")\n",
    "\n",
    "    try:\n",
    "        preprocess_category(review_tar, meta_tar, output_dir, category)\n",
    "\n",
    "        review_df = convert_to_dd(os.path.join(output_dir, \"reviews_parquet\"), category)\n",
    "        meta_df = convert_to_dd(os.path.join(output_dir, \"meta_parquet\"), category)\n",
    "\n",
    "        if review_df is not None and meta_df is not None:\n",
    "            cleaned = clean_data_dask(category, review_df, meta_df)\n",
    "            cleaned_all_categories[category] = cleaned\n",
    "            print(f\"Cleaned and stored: {category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {category}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        for sub in [\"reviews_parquet\", \"meta_parquet\", \"temp_extract\"]:\n",
    "            path = os.path.join(output_dir, sub)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    shutil.rmtree(path)\n",
    "                    print(f\"Deleted: {path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Couldn't delete {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cleaned_df = dd.concat(cleaned_all_categories.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8152e7",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Dask-compatible EDA...\")\n",
    "\n",
    "  # Ensure ddf is defined before using it\n",
    "if combined_cleaned_df is not None:\n",
    "    # Star Rating Histogram (1â€“5)\n",
    "    rating_counts =  combined_cleaned_df[\"rating\"].value_counts().compute().sort_index()\n",
    "    rating_counts.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Star Rating\")\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    plt.title(\"Distribution of Star Ratings\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: 'ddf' is not defined. Please define 'ddf' before running this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c094f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_cleaned_df is not None:\n",
    "    top_categories = combined_cleaned_df[\"main_category\"].value_counts().compute().head(10)\n",
    "    top_categories.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Main Category\")\n",
    "    plt.ylabel(\"Review Count\")\n",
    "    plt.title(\"Top 10 Categories by Review Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62879e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"brand\" in cleaned.columns:\n",
    "    top_brands = combined_cleaned_df[combined_cleaned_df[\"brand\"] != \"Unknown\"][\"brand\"].value_counts().compute().head(10)\n",
    "    top_brands.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Brand\")\n",
    "    plt.ylabel(\"Review Count\")\n",
    "    plt.title(\"Top 10 Brands by Review Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"year\" in combined_cleaned_df.columns:\n",
    "    yearly_avg = combined_cleaned_df.groupby(\"year\")[\"rating\"].mean().compute()\n",
    "    yearly_avg.plot(kind=\"line\", marker='o')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Average Rating\")\n",
    "    plt.title(\"Average Rating Over Time\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"review_length\" in combined_cleaned_df.columns:\n",
    "    corr = combined_cleaned_df[[\"review_length\", \"rating\"]].corr().compute()\n",
    "    print(f\"Pearson correlation between review length and rating: {corr.loc['review_length', 'rating']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
