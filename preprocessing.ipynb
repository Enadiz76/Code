{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bc8b9",
   "metadata": {},
   "source": [
    "# COMP 3610 – A3\n",
    "\n",
    "- Zidane Timothy, Maia Neptune, Christophe Gittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dda1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark\n",
    "# %pip install findspark\n",
    "# %pip install -q gdown\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install pyarrow\n",
    "# %pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b23bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "# import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# import `StandardScaler`\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# sudo apt install python3-distutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import time, matplotlib.pyplot as plt, seaborn as sns, matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80463dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder\\\n",
    "# .appName(\"Amazon_Reviews\")\\\n",
    "# .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"parent_asin\", FloatType(), True),\n",
    "    StructField(\"user_id\", ArrayType(StringType()), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True),\n",
    "    StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    StructField(\"helpful_vote\", StringType(), True),\n",
    "])\n",
    "\n",
    "# String types in arrays may need to be sequence but couldn't find the actual sequence dytpe syntax\n",
    "meta_schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", FloatType(), True),\n",
    "    StructField(\"rating_number\", IntegerType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType()), True),\n",
    "    StructField(\"description\", ArrayType(StringType()), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True),\n",
    "    StructField(\"videos\", ArrayType(StringType()), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"categories\", ArrayType(StringType()), True),\n",
    "    StructField(\"details\", MapType(StringType(), IntegerType()), True),\n",
    "    StructField(\"parent_asin\", FloatType(), True),\n",
    "    StructField(\"user_id\", ArrayType(StringType()), True),\n",
    "    StructField(\"bought_together\", ArrayType(StringType()), True),\n",
    "    # StructField(\"timestamp\", IntegerType(), True),\n",
    "    # StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    # StructField(\"helpful_vote\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_folder = 'root/Data'\n",
    "output_folder = 'root/output_folder'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8871d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_bz2(tar_path, extract_dir):\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Error: File {tar_path} does not exist.\")\n",
    "        return\n",
    "    if not tar_path.endswith(\".tar.bz2\"):\n",
    "        print(f\"Error: File {tar_path} is not a .tar.bz2 file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "            print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "            tar.extractall(path=extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_category(review_tar_path, meta_tar_path, output_folder, category,batch_size=1000):\n",
    "    temp_path = \"root/Data/temp_extract\" # change as needed\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"Extracting tar files...\")\n",
    "    extract_tar_bz2(review_tar_path, temp_path)\n",
    "    extract_tar_bz2(meta_tar_path, temp_path)\n",
    "\n",
    "    arrow_files = list(Path(temp_path).rglob(\"*.arrow\"))\n",
    "    print(f\"Found {len(arrow_files)} Arrow files\")\n",
    "\n",
    "    for arrow_file in arrow_files:\n",
    "        try:\n",
    "            is_meta = \"meta\" in str(arrow_file).lower()\n",
    "            folder_name = \"meta\" if is_meta else \"reviews\"\n",
    "\n",
    "            pkl_output_path = os.path.join(output_folder, f\"{folder_name}_pkl\")\n",
    "            os.makedirs(pkl_output_path, exist_ok=True)\n",
    "\n",
    "            # print(f\"Streaming {arrow_file.name} → {parquet_output_path}\")\n",
    "            dataset = load_dataset(\"arrow\", data_files=str(arrow_file), split=\"train\", streaming=True)\n",
    "\n",
    "            batch = []\n",
    "            seen_keys = set()\n",
    "            batch_num = 0\n",
    "\n",
    "            for i, row in enumerate(dataset):\n",
    "                if not row:\n",
    "                    continue\n",
    "\n",
    "                if not is_meta:\n",
    "                    key = (row.get(\"user_id\"), row.get(\"asin\"), row.get(\"text\"))\n",
    "                    if key in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(key)\n",
    "\n",
    "                batch.append(row)\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    table = pa.Table.from_pylist(batch)\n",
    "                    # pq.write_to_dataset(table, root_path=parquet_output_path)\n",
    "\n",
    "                    # convert to pandas and save as .pkl batch\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df.to_pickle(os.path.join(pkl_output_path, f\"{category}batch{batch_num}.pkl\"))\n",
    "                    print(f\"Saved batch {batch_num} ({len(batch)} rows) to .pkl\")\n",
    "                    batch = []\n",
    "                    batch_num += 1\n",
    "\n",
    "            # Final batch\n",
    "            if batch:\n",
    "                table = pa.Table.from_pylist(batch)\n",
    "                # pq.write_to_dataset(table, root_path=parquet_output_path)\n",
    "\n",
    "                df = pd.DataFrame(batch)\n",
    "                df.to_pickle(os.path.join(pkl_output_path, f\"{category}batch{batch_num}.pkl\"))\n",
    "                print(f\"Saved final batch {batch_num} ({len(batch)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {arrow_file.name}: {e}\")\n",
    "\n",
    "    shutil.rmtree(temp_path)\n",
    "    print(\"All done, temp folder removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872f022",
   "metadata": {},
   "source": [
    "Calling fn to preprocess for a category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a80c9",
   "metadata": {},
   "source": [
    "Run for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88009925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_category(\n",
    "#     r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_meta_All_Beauty.tar.bz2\",\n",
    "#     r\"D:\\UWI\\Year 3\\Sem 2\\COMP3610-Big-Data\\Assignments\\Assignment#3\\A3\\datasets\\raw_review_All_Beauty.tar.bz2\",\n",
    "#     \"output_folder\", category=\"All_Beauty\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702fd76",
   "metadata": {},
   "source": [
    "Meta and Review parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(folder): #folder for pkl\n",
    "    df_r = []\n",
    "\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith(\".pkl\"):\n",
    "            try:\n",
    "                file_path = os.path.join(folder, fname)\n",
    "                review_df = pd.read_pickle(file_path)\n",
    "                print(f\"{fname} loaded: shape = {review_df.shape}\")\n",
    "                df_r.append(review_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {fname}:\", e)\n",
    "\n",
    "    if df_r:\n",
    "        full_review_Amazon_df = pd.concat(df_r, ignore_index=True)\n",
    "        print(\"All .pkl files loaded. Final shape:\", full_review_Amazon_df.shape)\n",
    "        return full_review_Amazon_df\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621964",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462347ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(\n",
    "#     review_df,\n",
    "#     meta_df,\n",
    "#     on=\"parent_asin\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06160c",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b90b0",
   "metadata": {},
   "source": [
    "Dealing with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(details, store):\n",
    "    try:\n",
    "        if isinstance(details, dict) and \"brand\" in details and details[\"brand\"]:\n",
    "            return details[\"brand\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(category, review_df, meta_df):\n",
    "    output_dir = r\"D:/UWI/Year 3/Sem 2/COMP3610-Big-Data/Assignments/Assignment#3/A3/datasets/output_folder/cleaned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Merging review and meta...\")\n",
    "    merged_df = pd.merge(review_df, meta_df, on=\"parent_asin\", how=\"inner\")\n",
    "    print(\"Merged\")\n",
    "\n",
    "    print(\"Filtering invalid ratings...\")\n",
    "    merged_df = merged_df[merged_df[\"rating\"].between(1.0, 5.0, inclusive=\"both\")]\n",
    "\n",
    "    print(\"Dropping empty review text...\")\n",
    "    merged = merged_df[merged_df[\"text\"].notna() & (merged_df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    print(\"Extracting brand from metadata...\")\n",
    "    merged[\"brand\"] = merged.apply(lambda row: extract_brand(row.get(\"details\"), row.get(\"store\")), axis=1)\n",
    "\n",
    "    print(\"Removing duplicate reviews...\")\n",
    "    merged.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    print(\"Computing review length...\")\n",
    "    merged[\"review_length\"] = merged[\"text\"].str.split().apply(len)\n",
    "\n",
    "    print(\"Extracting year from timestamp...\")\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{category}_cleaned_merged.pkl.bz2\")\n",
    "    merged.to_pickle(output_file, compression=\"bz2\")\n",
    "\n",
    "    print(\" All cleaning steps completed.\")\n",
    "    \n",
    "    test = merged.copy()\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52287f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned = clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5540f",
   "metadata": {},
   "source": [
    "Run for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [\"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\"]\n",
    "categories = ['Office_Products', 'Patio_Lawn_and_Garden', 'Pet_Supplies', 'Sports_and_Outdoors', \n",
    "              'Software','Subscription_Boxes', 'Tools_and_Home_Improvement', 'Toys_and_Games', 'Video_Games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0959625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# for category in categories:\n",
    "#     base_path = r\"/root/Data\"\n",
    "#     meta_path = os.path.join(base_path, f\"raw_meta_{category}.tar.bz2\")\n",
    "#     review_path = os.path.join(base_path, f\"raw_review_{category}.tar.bz2\")\n",
    "\n",
    "\n",
    "\n",
    "#     preprocess_category(meta_path, review_path, \"output_folder\", category)\n",
    "\n",
    "meta_df = convert_to_df('/root/Data/output_folder musical-video_games/meta_pkl')\n",
    "meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = convert_to_df('/root/Data/output_folder musical-video_games/reviews_pkl')\n",
    "review_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
